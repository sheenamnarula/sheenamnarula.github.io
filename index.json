[{"authors":["admin"],"categories":null,"content":"This is my website that is a bit copy of me. I am a Software Engineer possessing experience of 3 years as a backend developer(Node js).\nBeing from Electronics backround, I do have a decent knowledge of Electronics fundamentals and would love to go for IOT projects.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"http://sheenamnarula1993.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"This is my website that is a bit copy of me. I am a Software Engineer possessing experience of 3 years as a backend developer(Node js).\nBeing from Electronics backround, I do have a decent knowledge of Electronics fundamentals and would love to go for IOT projects.","tags":null,"title":"Sheenam Narula","type":"author"},{"authors":null,"categories":null,"content":"In this post, we will understand the different parts of configuration file of kubernetes by deploying a node application. For this post, you can find github code at :\nhttps://github.com/sheenamnarula/Node-mongo-kubernetes Refer this repository that contains dockerised node application code and configuration files for kubernetes deployment for better understanding.\nSecond Option : Create a node app and dockerise it. For dockerisation of node app, you can follow below post :\n http://sheenamnarula93.com/post/express-docker-aws-1/ Kubernetes Configuration File : Folder Location for files in cloned repository : k8s\n Node app file : k8s/node-app.yaml Mongo file : k8s/mongodb.yaml Explanation of files : apiVersion : apps/v1  This line describes the Kubernetes API versions to be used to create the resource.  kind : Deployment  This line describes the type of resource to be created.  metadata : labels : app : node-app name : node-app Metadata :  attaches meta-information regarding the resource such as its name or labels.  spec: replicas : 1 selector : matchLabels : app : node-app-pod template : metadata : labels : app : node-app-pod spec : containers : - name: node-application env : - name : DB_URI value : mongodb://mongodb-service:27017/node-application image : sheenam1993/node-application ports : - containerPort: 3007 restartPolicy: Always Spec  describes the specifications of resources  The deployment resources are used to manage and control the life cycle of applications Pods. As a result,the spec section for the deployment resource is containing information about the applications Pod and how to control them, below is a brief description of the basic sections that are required for Deployment Spec:\n  Replicas:   an integer that specifies how many pods the deployment should create.\n  Selectors:   this configuration section specifies the conditions used to select the Pods managed by Deployment. For instance, in the example that we are going to use, Pods that have the label app: mongodb-pod will be managed by the Deployment resource.\nTemplate : this section contains the configurations of the Pods.\nThe Template section contains the following subsections:\n  Metadata   The information that will be attached to every Pod created by our Deployment. The labels defined in this section must match the ones used in the selector sections so that the deployment can manage the Pods after their creation.\n  Spec   The specifications of the Pod containers (notice that containers sections is a list and can include more than one container definition). Here we can define specific container configurations such as the image name, the attached volumes, restart policy and used ports.\nService in Kubernetes apiVersion: v1 kind: Service metadata: labels: app: node-app-pod name: node-app-service spec: ports: - port: 8080 targetPort: 3007 nodePort: 30007 selector: app: node-app-pod type: NodePort A service makes pods available for interaction to other application pods in the cluster.\nExplanation of Service Code :\nkind: Service Kind describes the type of resource to be created. So while creating service, its value is \u0026ldquo;Service\u0026rdquo;\nmetadata: labels: app: node-app-pod name: node-app-service Metadata describes the information about the service. Here we have given two thing under meta data : labels and name.\nA service can be called by another service in cluster by name.\nspec: ports: - port: 8080 targetPort: 3007 nodePort: 30007 selector: app: node-app-pod type: NodePort Spec describes specifications of servoice. Here is description of spec code :\n  ports   ports contain 3 parts further : port : exposed port of service (for other services)\n  targetPort   exposed port of a pod (means on which our app is running)\n  nodePort   exposed port to access service from outside the cluster (means if we have to expose the service to end user to run, we have to use nodePort)\n  selector   the conditions used to select Pods managed by the service resource.\n  type :   Kubernetes supports a couple of service types for managing internal and external traffic. Since MongoDB should not be accessed by any external entity, we will create a ClusterIP service that will expose the service on a cluster-internal IP (only accessible by Pods in the same cluster).But for Node app we have created NodePort service type which can be accessed from outside the cluster as well.\n","date":1602786600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602786600,"objectID":"c2cea20281eedda7d274dd58974a0eb1","permalink":"http://sheenamnarula1993.github.io/post/kubernetes-part-3/","publishdate":"2020-10-16T00:00:00+05:30","relpermalink":"/post/kubernetes-part-3/","section":"post","summary":"In this post, we will understand the different parts of configuration file of kubernetes by deploying a node application. For this post, you can find github code at :\nhttps://github.com/sheenamnarula/Node-mongo-kubernetes Refer this repository that contains dockerised node application code and configuration files for kubernetes deployment for better understanding.\nSecond Option : Create a node app and dockerise it. For dockerisation of node app, you can follow below post :\n http://sheenamnarula93.","tags":null,"title":"Kubernetes Configuration file","type":"post"},{"authors":null,"categories":null,"content":"Get started with Kubernetes on local system To get started with kubernetes on your local system, you can use minikube. Minikube is basically local kubernets which helps to learn and develop kubernetes locally.All you need is Docker (or similarly compatible) container or a Virtual Machine environment\nInstallation process and important commands of minikube is given as below :\nInstallation of minikube on Linux curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux- sudo install minikube-linux-amd64 /usr/local/bin/minikube Command to Start cluster minikube start Command to Stop cluster minikube stop Command to Delete cluster minikube delete --all Minikube Dashboard There are commands to manage and access cluster but for ease, we will use dashboard provided by minikube where each and every detail of cluster can be seen and managed. To launch minikube dashboard, run the following command in terminal :\nminikube dashboard Install kubectl (Command line interface) The Kubernetes command-line tool, kubectl, allows you to run commands against Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs.\nsudo snap install kubectl --classic Commands to create kubernetes objects(after describing yaml files): kubectl apply -f ${fileName}.yaml kubectl create -f ${fileName}.yaml This was very basic understanding of minikube i.e local kubernetes. In the next part, we will look into the kubernetes configuration file. For this, we will deploy a Node application for better understanding of concepts. ","date":1602786600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602786600,"objectID":"dd9cf7b3b5cd646e2b11cb8a67e0c554","permalink":"http://sheenamnarula1993.github.io/post/kubernetes-part-2/","publishdate":"2020-10-16T00:00:00+05:30","relpermalink":"/post/kubernetes-part-2/","section":"post","summary":"Get started with Kubernetes on local system To get started with kubernetes on your local system, you can use minikube. Minikube is basically local kubernets which helps to learn and develop kubernetes locally.All you need is Docker (or similarly compatible) container or a Virtual Machine environment\nInstallation process and important commands of minikube is given as below :\nInstallation of minikube on Linux curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux- sudo install minikube-linux-amd64 /usr/local/bin/minikube Command to Start cluster minikube start Command to Stop cluster minikube stop Command to Delete cluster minikube delete --all Minikube Dashboard There are commands to manage and access cluster but for ease, we will use dashboard provided by minikube where each and every detail of cluster can be seen and managed.","tags":null,"title":"Kubernetes Local System Set up","type":"post"},{"authors":null,"categories":null,"content":"Kubernetes Vocab : Node : It is an instance of computer which is also known as worker nodes.\nPods : It is a smallest unit of kubernetes that we can deploy on kubernets cluster.It contain container\u0026rsquo;s image. Bascially a pod is a collection of more than one conatiners which are supposed run on one host logically.\nWhen a pod is created, an ip address is assigned to it.When a worker node dies, replica of that pod is created but with different ip address and different volumes. That means everything related to pod lifecycle dies with that pod.(stable ip address concept)\nServices : Services are the end points that export ports to outside world and map the ports to pods.\nDeployment : It represents a set of multiple identical pods.Deployments run multiple instances or replicas of your application.It ensures the availability of one or more instances of app to serve user requests.Deployments are managed by kubernetes deployment controller. Deployments use a Pod template, which contains a specification for its Pods. The Pod specification determines how each Pod should look like: what applications should run inside its containers, which volumes the Pods should mount, its labels, and more.\nWhen a Deployment\u0026rsquo;s Pod template is changed, new Pods are automatically created one at a time.\nKUBERNETES ARCHITECTURE : Kubernetes Master Components: Etcd, API Server, Controller Manager, and Scheduler : Kubernetes master is considered as the brain of kubernetes cluster. It run Etcd, Scheduler, Controller Manager and Api Server inside it.Lets discuss there components briefly so that working on kubernetes should not appear to be strange and knowing components of kubernetes master will help us in a significant way in creating a working image of kubernetes master.\nEtcd : Etcd is a very important part of kubernetes master node. It saves the configuration data of kubernetes cluster like which nodes are there,how many pods are there and which pod is running on which node and a lot more information regarding cluster state. Kubernetes master reads and write data to etcd to get the current state and decides the required logical operations to be performed to achieve the desired state. Etcdctl is a command line interface to manage Etcd through which many operations for etcd can be performed like adding and removing key-value pairs(etcd data storage format is key value pair), verify cluster health, generating database snapshots(As Kubernetes master is highly dependent on etcd for data, so we should have a backup plan for etcd. We can save a snapshot of etcd by using etcdctl save snapshot command).\nEtcd also impelements a \u0026ldquo;watcher\u0026rdquo; feature in which for every key there is a watcher. Whenever key changes, watcher is notified and in return an event is triggered for Api server. After listening to event, api server takes important decisions of operations to be performed to move current state towards desired state.\nApi Server : (The REST API is the fundamental fabric of Kubernetes. All operations and communications between components, and external user commands are REST API calls that the API Server handles. Consequently, everything in the Kubernetes platform is treated as an API object and has a corresponding entry in the API.) Kubernetes api server is a main management point of kubernetes cluster.Basically when we interact with kubernetes using kubectl command line interface, we are actually communicating with Api server component. Only Api server component directly communicates with Etcd to read, write and update objects after validating requests. Rest all components has to go through Api server to work with cluster state. The API Server also implements a watch mechanism (similar to etcd) for clients to watch for changes. This allows components such as the Scheduler and Controller Manager to interact with the API Server in a loosely coupled manner. For example :\n kubectl writes to api server Api server writes to etcd after validating request. etcd notifies back to api server. Api server invokes scheduler. Scheduler decides where to run the pod on and return back to Api server. Api persists data in etcd. Etcd writes back to api server. Api server invokes the Kubelet in corresponding node. Kubelet talks to docker daemon using the api over docker socket to create continer. Kubelet updates the status to Api server. Api server interacts with etcd to persist new state.  Controller Manager : Controller manager watches the cluster state through Api server and when it gets notified, it makes necessary changes to move current state of cluster towards desired state.For e.g. Replication controller always maintains the required number of replicas of pods.\nScheduler : Scheduler watches for unscheduled pods and decides as which pod has to run on which node. Once a pod\u0026rsquo;s binding to node is done, regular task of Kubelet is invoked which in turn calls container engine like docker to create and start container.\nKubernetes Node Components : Basically, Kubernetes node runs Kubelet and Service Proxy components. It also runs container engine like Docker which in turn runs containerized applications.\nKubelet : It\u0026rsquo;s an agent that runs on each node and being a watcher it reports activities of pods to api server.It makes sure that pods attached to its node are running. It talks to docker daemon through docker socket api and which in turn is responsible for running containers.\nJobs of kubelet :\n Run the pods. Report the status of node and pods to api server. Run container probes.  cAdvisor The Kubelet ships with built-in support for cAdvisor, which collects, aggregates, processes and exports metrics (such as CPU, memory, file and network usage) about running containers on a given node. cAdvisor includes a built-in web interface available on port 4194 (just open your browser and navigate to http://:4194/).\nYou can view the cAdvisor /metrics endpoint by issuing a GET request to http://:4194/metrics.\nService Proxy : Service Proxy runs on each node and is responsible for watching the api server for pods and services definitions' changes to keep the entire network\u0026rsquo;s configuration up to date,ensuring that one pod can talk to another pod,one node can talk to another node, one container can talk to another container.In addition to this, it maintains all ip route tables so that it can redirect user request to a correct backend resource. That is why, when we hit the Node port, even if app is not available on that node but it redirects the request to the correct backend resource for execution.\n","date":1602700200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602700200,"objectID":"65197beba3fe29b1ae29ee33b3571e2e","permalink":"http://sheenamnarula1993.github.io/post/kubernetes-part-1/","publishdate":"2020-10-15T00:00:00+05:30","relpermalink":"/post/kubernetes-part-1/","section":"post","summary":"Kubernetes Vocab : Node : It is an instance of computer which is also known as worker nodes.\nPods : It is a smallest unit of kubernetes that we can deploy on kubernets cluster.It contain container\u0026rsquo;s image. Bascially a pod is a collection of more than one conatiners which are supposed run on one host logically.\nWhen a pod is created, an ip address is assigned to it.When a worker node dies, replica of that pod is created but with different ip address and different volumes.","tags":null,"title":"Kubernetes Architecture","type":"post"},{"authors":null,"categories":null,"content":"Syntax of import/export : file : exportVar.js\nexport const m = 1 ; file : importVar.js\nimport {m} from exportVar.js Syntax of require : file : exportVar.js\nconst m = 1; module.exports = m ; file: importVar.js\nconst m = require('./exportVar.js') Loading Strategy : Mainly import/export i.e. ES6 structure, involves asynchronous loading of modules. But in case of require(), synchronous loading of modules takes place. It means, if a file needs to load 5 modules, in case of ES6 structure, second module won\u0026rsquo;t wait for first module to be loaded completely.But in case of require, loading of second module will start after loading of first module is completed.\nThis is the key difference between import/export and require.\nTo get more deep in to this, we need to know how ES6 structure is working and how it is different from Common js.\nThe key difference between the Common JS and ES6 modules is when to know the shape of module.\nIn Common JS, when a module is exported, the steps performed before execution is given in this figure.\nHere the common mistake that most of the developers make in understanding the loading step. In loading step, it is determined that what kind of thing is pointed by absolute path that is obtained on completion of resolution step.If it is Node.js native module, then dynamic linking of module is decided. On the other hand, if it is JSON file, the content is loaded in memory for further use. Here is the trick, in wrapper step, before evaluation, the loaded Javasacript is wrapped in a function. For e.g.\nIn file, example.js\nconst m = 1; module.exports.m = m; code is actually wrapped and passed to evaluation in below function format :\nfunction (exports, require, module, **filename, **dirname) { const m = 1; module.exports.m = m; } Here the exports is a parameter for wrapper function and it is a normal javascript object. So when the wrapper function is evaluated, it returns the exports object which is then used as a result of require(). The main point to note here is that, there is no way to determine as what kind of module is being exported by Common JS until the evaluation of wrapper.\nOn the other hand, if we use import/export i.e. ES6 feature, the equivalent code of above example is :\nIn file, example.js\nexport const m = 1; import {m} from './example.js' In this case, the shape of m is actually determined at the parsing step i.e before evaluation step. Actually, while parsing, and before evaluation, a Module Record is created, where a static listing of modules that have been exported in code are listed and verified. That means, it is verified first whether the exported m exists or not and link between import file and exported module i.e. m is eastablished. Only after creation of this module record, code is actually evaluated. Here the key point is, before even evaluation of code, the shape of module is known or we can say that, import/export is resolved before evaluation of code.\nChallenge for Node.js : Here is the challenge for Node.js comes up when exported module is not an ESM i.e ECMASCRIPT Module but a Common JS module because while importing with import statement or according to ES6 structure, shape of module should be known before evaluation but in case of Common JS module it becomes impossible to determine shape of code before evaluation step due to wrapper function.\nTo resolve this challenge, one proposal came up in which it was proposed that if a module is ESM module, then it will be directly added to list of Module Record but if it is not i.e if it is a Common JS module, then rather than giving error or giving up, the verification step will be left in pending state and a process of evaluation of that module will be executed. After getting confirmation of existence of that module, Module Record will be created.\nBut again for the implementation of this proposal, Node js requires some mechanism to identify whether the module is ESM or Common JS Module. Various approaches have been suggested but Michael Jackson Script is supposed to be accepted.\nMichael Jackson Script : In this a new file extension *.mjs is supposed to be introduced for ESM modules. Right now, Node is distinguishing between the modules like this :\n.node files : native modules\n.js : Common JS modules\n.json : JSON files\nNow *.mjs will specify that the module is ESM and it should be loaded with the right mechanism. The key point here is by knowing the extensions of file, loading mechanism will be selected. For Common JS module, evaluation of module will be done first so that verification of existence can be done.\nE.g. There are two files common.js and esm.mjs\nimport c from './common.js' will be treated as Common js module\nimport e from './esm.mjs' will be treated as ESM module\nWhat to do if i want to use ES6 import/export feature in project? Answer is Babel If we want to use import/export feature of ES6, we need to use babel which transpiles code in Common JS. Here is a point which needs to be understood that Babel only converts the syntax to Common JS syntax so that it can be resolved by Node js, but implementation is according to ESM only.\nExample: If we see named imports like\nimport {a,b} from 'ab'; This syntax will work only if ab is an ESM. But if it is Common JS module, it is not possible to use this syntax as shape of code can\u0026rsquo;t be determined until evaluation is done in this case.\nBut here BABEL does a perfect job for us by converting ES6 syntax to Common JS syntax and we get the required implementation.\nSignificance of using ES6 structure : In case of ES6 structure, the first step of loading the contents of file from disk is quite similar but may happen asynchronously. When the content of files is available, they are parsed. While parsing, the shape of exported module is determined and linked to respective import.That means all exports and imports will know their targets before evaluation itself i.e. they are resolved before execution step and that too happen asynchronously. In node.js terms, we can say that loading scripts, resolution of import and export and evaluation of module code occur over multiple turns of the event loop.\nPractical Example : module a and b, both does not exist. b is imported using require and a is imported using import. Now due to use of require, existence of b is not verified and code does not throw error for non-existence of b. But in case of module a, existence of a is verified and code throws error for a at very early stage i.e at parsing step only.\n","date":1564511400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564511400,"objectID":"8a9034665e184fa7b13bc43449ada527","permalink":"http://sheenamnarula1993.github.io/post/import-require/","publishdate":"2019-07-31T00:00:00+05:30","relpermalink":"/post/import-require/","section":"post","summary":"Syntax of import/export : file : exportVar.js\nexport const m = 1 ; file : importVar.js\nimport {m} from exportVar.js Syntax of require : file : exportVar.js\nconst m = 1; module.exports = m ; file: importVar.js\nconst m = require('./exportVar.js') Loading Strategy : Mainly import/export i.e. ES6 structure, involves asynchronous loading of modules. But in case of require(), synchronous loading of modules takes place. It means, if a file needs to load 5 modules, in case of ES6 structure, second module won\u0026rsquo;t wait for first module to be loaded completely.","tags":null,"title":"Import Vs Require","type":"post"},{"authors":null,"categories":null,"content":"Containerization : Containerization is a lightweight alternative to full machine virtualization that involves encapsulating an application in a container with its own operating environment. Containers use the same host operating system (OS) repeatedly, instead of installing an OS for each guest VM.\nVirtualization : A virtual machine is a copy of a complete server basically it has its own OS which is treated as a guest OS on host OS.\nUse case of Containers : Suppose we have two applications written on different versions of same language.It is possible that if we deploy both on same VM, one of those may break due to unsupported version of language.But if we deploy them in Containers,every container will have the required version and both apps will run in isolated environments.\nSecurity Level : Virtual Machines are fully isolated and hence more secure. But in case of containers, process-level isolation and hence less secure.\n","date":1563561000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563561000,"objectID":"bd227681f4b1cb3f0b932d1d56759e4a","permalink":"http://sheenamnarula1993.github.io/post/containerization-vs-vm/","publishdate":"2019-07-20T00:00:00+05:30","relpermalink":"/post/containerization-vs-vm/","section":"post","summary":"Containerization : Containerization is a lightweight alternative to full machine virtualization that involves encapsulating an application in a container with its own operating environment. Containers use the same host operating system (OS) repeatedly, instead of installing an OS for each guest VM.\nVirtualization : A virtual machine is a copy of a complete server basically it has its own OS which is treated as a guest OS on host OS.","tags":null,"title":"Containerization Vs Virtualization","type":"post"},{"authors":null,"categories":null,"content":"I am assuming that an amazon account is already set up to use the Amazon Elastic Container Service(ECS).\nSteps to deploy two containers(app + database) -\n Dockerise app Create Registry(ECR) Upload app image to ECR Create task definition with two containers Create a cluster Create a service and run it.  Practical Explanation of steps : 1. Dockerise app for this you can check this practice project : https://github.com/sheenamnarula/node-graphql-docker-aws\n2. Create Registry(ECR) We need to keep image of app somewhere from where it is pulled while starting container.We can do two things here. Either we can use Docker Hub or we can use AWS Elastic Container Registry(ECR) service. In this tutorial,i am using ECR to keep the created images.\nLog in to web console into your prefered region and choose ECR from services.\nThere create a repository to store image of your app.After creating repository, you will see a button \u0026ldquo;View Push Commands\u0026rdquo; on right side. On clicking that button, you will get some commands that will be used to push image of app to this repository. 3 . Upload image to ECR Run the commands, to push the image to ECR.\nCommand 1.  aws ecr get-login --no-include-email --region ap-south-1  This will help you to login to aws through cli.(Note : Install aws-cli to run these commands.) ( Link to install aws cli : https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html )\nCommand 2.  docker build -t node-express .  This command will help you in building image of your app. Run this command in the folder containing the app. -t is to give the name to your image. You can also customize it according to your preference so that you can find the created image easily. Command 3.  docker tag node-express:latest 597357263415.dkr.ecr.ap-south-1.amazonaws.com/node-express:latest  This command is used to tag your created image to the ECR. Command 4 .  docker push 597357263415.dkr.ecr.ap-south-1.amazonaws.com/node-express:latest  This command is used to push the created image to ECR. This command can take few minutes to execute. 3. Create Task definitions As we run docker commands like docker run in docker cli, here task definitions are used for this purpose. Every task consists of - docker containers details, port mappings, network details, environment variables, volumes if any, and the most important thing, image of app. Choose Task Definitions in left navigation pane.\nHere we will define two containers in one task as running app willl contain api container and database container.\nFirst Container : Api Container Copy the url of your image that we pushed in ECR in second step.\nClick on Create New Task Definition and choose EC2 and then click on next. Here you will get a form to define your task. Fill out the required fields and click on Add Containers. In add container form, specify the container name and memory required. Then give the port mappings i.e host port and container port. Give Environment variables using Key-value pair(In this project, we are using mongo db as environment variable) In network links, you can declare the linking of other containers with this container simply by following syntax - othercontainer: alias Here we are running database in second container and we are gonna name it as mongo-container(yet to be defined) We are linking this container to database container. Then click on create and you will get your api container created.\nSecond Container : Database Container For this container, we need a mongo image and this can be directly pulled from Docker Hub. Here is the required url for mongo image : registry.hub.docker.com/library/mongo:latest\nNow we need to be careful while naming second container as we have mentioned in the linking of first container as \u0026ldquo;mongo-container\u0026rdquo;. So for correct linking, we need to keep the name of our db container as \u0026ldquo;mongo-container\u0026rdquo;\nand also mention the hostname as mongo-container.\nNow click on create option and you will get your task definition created with two containers.\n4. Create Cluster : Cluster is where container runs.\nChoose Clusters from left navigation pane and choose create cluster.Creating cluster is like setting up an EC2 instance.After clicking on create cluster, choose EC2+Linux and create cluster with options vpc - create new vpc group,instance type- m4.large and number of required instances. Configuration of cluster : Now you can see the created cluster, on clicking that cluster, you can see tabs for task definition, ec2 instance and services.\n5. Create Service : Service is used to run your defined tasks in cluster. Click on create service and fill the required fields.\nCreating service : Choose the task definition that have been created in step 3 and run the service.\nInitially it will show pending state and soon it will be in running state. When it is in running state, you can go to cluster and then click on ec2 instance. On clicking that, details of that instance will appear. From there copy the public dns. Paste the copied address in browser and you can see your app in running state. ","date":1561746600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561746600,"objectID":"422a0a7a75c8756d4e8ac358bbdc6623","permalink":"http://sheenamnarula1993.github.io/post/express-docker-aws-2/","publishdate":"2019-06-29T00:00:00+05:30","relpermalink":"/post/express-docker-aws-2/","section":"post","summary":"I am assuming that an amazon account is already set up to use the Amazon Elastic Container Service(ECS).\nSteps to deploy two containers(app + database) -\n Dockerise app Create Registry(ECR) Upload app image to ECR Create task definition with two containers Create a cluster Create a service and run it.  Practical Explanation of steps : 1. Dockerise app for this you can check this practice project : https://github.com/sheenamnarula/node-graphql-docker-aws","tags":null,"title":"Node-Docker-Aws Part-2","type":"post"},{"authors":null,"categories":null,"content":"First of all, make a project in node-express-mongo and dockerise it. Here I am giving a github link of a basic node-express app made along with graphql as this has become my favourite language because of its advantage of providing independence to front-end developers.\nGithub link for dockerised app : https://github.com/sheenamnarula/node-graphql-docker-aws Docker File Lets understand first, what is the meaning of lines written in the docker file.\nFrom node:8.9.0 This line specifies the base image.Every docker file starts with a FROM instruction. As we are writing here for node app, so we need to set a node environmnent for the app. Here node:is the base image along with the required version for our app ie. 8.9.0\nWORKDIR /app This line refers to a directory in which we are going to keep our application files.Basically WORKDIR sets a directory where another commands like COPY, RUN, CMD, ADD can be run. In short, it is providing an entry point of app directory.\nCOPY package\\*.json ./ To start a project or run a project, we need to install node modules and as we know in a node app, package.json is a file where dependencies of project is documented. So we are copying package.json in working directory so that further instructions can install node modules using this file. Here package* refers to package.json and package.lock.json files.\nRUN npm install RUN npm install nodemon babel-cli babel-preset-env These both lines contain RUN command. After copying package.json in work directory that is suppose to be entry point of our source files,using RUN command we are installing node modules and required dependencies for our project. Basically RUN command is used for the command that we run in a terminal and then it commits a new image which is further used by next instructions of docker file\nEXPOSE 3007 This line is very important to understand as it defines which port of container is exposed.When we give any port with this command, that means that port of container is exposed.Means if we want to access app, we have to write the hostname simply i.e we need to write port 80 on browser. But if we write publish port in docker run command with expose port that means the app will be accesed through published port. In short, EXPOSE is used to open port for network but publish is used to map the opened port to public accessible port.\nCOPY . /app Here we are copying the all content of app to our container working directory so that app can be run.\nCMD npm run start This line refers to executing the command that is required to start the project.\nDifference between RUN and CMD - Here is an important difference between RUN and CMD :\nRUN - Basically RUN command triggers when we are building docker image. After RUN command, image is committed and next command uses that committed image.\nCMD - Basically CMD command triggers when we launch the created docker image.Dockerfile can have only one CMD command and it can be overriden while starting a container i.e. executing docker run $image $other_command\nThis is all about a docker file. Environment variables can also be included using ENV command. Now this is all about one container which will contain app. But here database required is mongo db and we also need a container for mongo db.\nNOTE : It is possible to run mongo db and app in one container but it will create problem in scaling because if we need to scale app, thereby we will be scaling mongo db as well and moreover, it is difficult to maintain same data availability for all container apps in this case. So we will have one container for mongo db service following microservice architecture i.e. every service is running independently.\nTo manage more than one container we will go for a tool named Compose. Compose is a tool that is used to define and run multi-container docker applications.In this, a YAML file is used to configure multiple containers of application services.Then with a single command, we can create and start all the services using yaml configuration file.\nHere is the code for docker compose file and explanation of each line of code :\nExplanation of Docker Compose file : version: \u0026quot;3\u0026quot; - This line refers to the docker compose file format version.\nservices: This line refers to the start of container configurations.A service definition contains configuration that is applied to each container started for that service.\napp: This line refers to the name of your service.\ncontainer_name: docker-node-mongo By default, a random name is generated. To generate a meaningful name, we use container_name parameter in config.\nrestart: always By default behaviour of container is that it never restarts. By setting restart to always, we are making container to opt restarting behaviour.\nbuild: . This option is used when we have to make an image from a docker file. Relative path is given in this option so that docker can look for Dockerfile according to given path to build image.\\\nports: - \u0026quot;80:3007\u0026quot; This option is used to mention the exposed port of container. HOST:CONTAINER is short syntax to metion ports.\nThe long form syntax allows the configuration of additional fields that can’t be expressed in the short form.\ntarget: the port inside the container published: the publicly exposed port protocol: the port protocol (tcp or udp) mode: host for publishing a host port on each node, or ingress for a swarm mode port to be load balanced.\nports: -target: 80 published: 8080 protocol: tcp mode: host depends_on: - mongo This line express the dependencies of one container to other. To start a container,the containers on which it is dependent, will be started first\nmongo: container_name: mongo image: mongo This will check the mongo image if it is available locally and in case of unavailability, image will be downloaded from docker hub(github for docker images).\nports: - \u0026quot;27017:27017\u0026quot; Here is on more command like depends_on i.e. links. But link is deprecated now. depends_on gives us start order but links just to link one container to another.But now, if containers are placed in same network, they can connect with each other by using their name.\nNow by running a single command, \u0026ldquo;docker-compose up\u0026rdquo;, we can see two containers running one is our app container and other is mongo container.\nNote: In project here is one commented line , mongodb://mongo:27017/expGraphqlDemo. In this mongo is container name and 27017 is container\u0026rsquo;s published port.But for deployment purpose, we are making it configurable and will pass as environment variable while deployment.\n","date":1561660200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561660200,"objectID":"9e0e13f0dc0a854765ba2c500efea3d1","permalink":"http://sheenamnarula1993.github.io/post/express-docker-aws-1/","publishdate":"2019-06-28T00:00:00+05:30","relpermalink":"/post/express-docker-aws-1/","section":"post","summary":"First of all, make a project in node-express-mongo and dockerise it. Here I am giving a github link of a basic node-express app made along with graphql as this has become my favourite language because of its advantage of providing independence to front-end developers.\nGithub link for dockerised app : https://github.com/sheenamnarula/node-graphql-docker-aws Docker File Lets understand first, what is the meaning of lines written in the docker file.\nFrom node:8.9.0 This line specifies the base image.","tags":null,"title":"Node-Docker-Aws Part-1","type":"post"},{"authors":null,"categories":null,"content":"Show commands and Management commands  $ docker  Docker version :  $ docker --version  Show info like number of containers  $ docker info  Commands required for Containers Command to run a container in foreground :  $ docker container run -it -p 80:80 {image name}  Example : docker container run -it -p 80:80 nginx\nCommand to run a container in backround :  $ docker container run -d -p 80:80 {image name}  Short hand command  $ docker container run -d -p 80:80 {image name}  Naming custom names to container  $ docker container run -p 80:80 --name {container name} {image name}  Example : docker container run -d -p 80:80 \u0026ndash;name nginx-server nginx\nPurpose of \u0026ldquo;run\u0026rdquo; in command :  It actually looks for image in local cache. If image is not found, it looks into Docker Hub where default image repo may exist(its like github for docker images). It pulls it down from there and stores into local image cache. Starts in a new container. As we know syntax for port specification is HOST:CONTAINER port. So here we can access on port 80. We can also access it on any port by changing host port. For e.g. , if we want access on port 3000, then we need to write here 3000:80.  Commands for listing containers These two commands gives us running containers.\n $ docker ps $ docker container ls  This command will give us all containers(including those containers as well which are not running).\n $ docker container ls -a  Commands to stop container Stop a specific container  $ docker container stop [ID]  Stop all containers  $ docker stop $(docker ps -aq)  Remove a specific container  $ docker container rm -f [ID]  Remove multiple containers  $ docker container rm [ID] [ID] [ID]  Remove all containers  $ docker rm $(docker ps -aq)  Get logs of container  $ docker container logs [NAME]  List processes running in container  $ docker container top [NAME]  View Container Info  $ docker container inspect [NAME]  Performance stats  $ docker container stats [NAME]  Commands required for Images List images  $ docker images  Build Image  $ docker image build -t [REPONAME] .  Pull Image  $ docker pull [IMAGE]  Remove a specific image  $ docker image rm [IMAGE]  Remove all images  $ docker rmi $(docker images -a -q)  Image tagging and pushing to docker hub Tags are labels that point to an Image\nTagging an Existing Image  $ docker image tag {old name} {new tag name}  Upload to docker hub  $ docker image push {imageTag}  Note : If pushing operation is denied, do login in docker using  $ docker login  Docker Networks Get Port  $ docker container port [NAME]  List Networks  $ docker network ls  Inspect Network  $ docker network inspect [NETWORK_NAME]  Create network $ docker network create [NETWORK_NAME]  Create container on network $ docker container run -d --name [NAME] --network [NETWORK_NAME] [IMAGE_NAME]  Connect existing container to network $ docker network connect [NETWORK_NAME] [CONTAINER_NAME]  Disconnect container from network $ docker network disconnect [NETWORK_NAME] [CONTAINER_NAME]  Detach network from container $ docker network disconnect  ","date":1561314600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561314600,"objectID":"3141febc905c62a73d557fc6f4987809","permalink":"http://sheenamnarula1993.github.io/post/docker-basic-commands/","publishdate":"2019-06-24T00:00:00+05:30","relpermalink":"/post/docker-basic-commands/","section":"post","summary":"Show commands and Management commands  $ docker  Docker version :  $ docker --version  Show info like number of containers  $ docker info  Commands required for Containers Command to run a container in foreground :  $ docker container run -it -p 80:80 {image name}  Example : docker container run -it -p 80:80 nginx\nCommand to run a container in backround :  $ docker container run -d -p 80:80 {image name}  Short hand command  $ docker container run -d -p 80:80 {image name}  Naming custom names to container  $ docker container run -p 80:80 --name {container name} {image name}  Example : docker container run -d -p 80:80 \u0026ndash;name nginx-server nginx","tags":null,"title":"Basic Docker Commands","type":"post"},{"authors":null,"categories":null,"content":".idea/— contains IntelliJ’s project specific settings files. According to JetBrains, normally there is no need to edit the contents of this folder.\nbuild.sbt — contains build definition of the project. For example, project name, project version, scalaVersion, libraryDependencies, etc. Note that this file is written in Scala.\nproject/—because build.sbt is written in Scala, we need to build build.sbt. Therefore, this folder contains all the files needed for building build.sbt.\nproject/build.properties — this file contains build definition of build.sbt. For example, it defines sbt version.\nproject/target/— contains artifacts from building build.sbt.\nsrc/—this is where we put our source files. sbt uses the same directory structure as Maven for source files by default. Other directories in src/ will be ignored in build time.\nsrc/main/scala — contains main .scala files\nsrc/main/java — contains main .java files\nsrc/main/resources — contains main files other than .scala/.java files\nsrc/test/scala — contains test .scala files\nsrc/test/java — contains test .java files\nsrc/test/resources — contains test files other than .scala/.java files\ntarget/— contains artifacts from building the project\nprintln(\u0026ldquo;Hello World\u0026rdquo;) Where should we chuck it?\nNormally when you run a Scala sbt project, it will use the main function as the entry point to run the project. So another question — where should we chuck the main function?\nBecause sbt needs to call the main function from an object, the answer is that we write it in a Scala object like this:\nobject HelloWorldMain { def main(args: Array[String]): Unit = { println(\u0026ldquo;Hello World\u0026rdquo;) } } So, let’s create a package called com.helloworld (alternatively you can use your domain name in reverse order) in src/main/scala.\nThen right-click on the package and select New -\u0026gt; Scala Class. We will name the class HelloWorldMain. Before you click OK, remember to change Kind from Class to Object for the reason we mentioned earlier.\nYou can add the code snippet we discussed earlier in the file.\nBuild and Run the project Now you should be able to see two green arrows next to your main function. Clicking either of them will build and run the project.\nThen you will see the “Hello World” text printed out in the Run tool window in Intellij.\nAnd the build artifact can be found in target/scala-2.12/classes.\n","date":1560969000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560969000,"objectID":"079d6dd991ce0dcaf29ec1d80eb85577","permalink":"http://sheenamnarula1993.github.io/post/scalaprojectstructure/","publishdate":"2019-06-20T00:00:00+05:30","relpermalink":"/post/scalaprojectstructure/","section":"post","summary":".idea/— contains IntelliJ’s project specific settings files. According to JetBrains, normally there is no need to edit the contents of this folder.\nbuild.sbt — contains build definition of the project. For example, project name, project version, scalaVersion, libraryDependencies, etc. Note that this file is written in Scala.\nproject/—because build.sbt is written in Scala, we need to build build.sbt. Therefore, this folder contains all the files needed for building build.sbt.\nproject/build.properties — this file contains build definition of build.","tags":null,"title":"Scala Part-2","type":"post"},{"authors":null,"categories":null,"content":"SBT is a popular tool for compiling, running, and testing Scala projects of any size. Using a build tool such as sbt (or Maven/Gradle) becomes essential once you create projects with dependencies or more than one code file.\nWhen you write small programs that consist of only one, or just two or three source files, then it\u0026rsquo;s easy enough to compile those source files by typing scalac MyProgram.scala in the command line.\nBut when you start working on a bigger project with dozens or maybe even hundreds of source files, then it becomes too tedious to compile all those source files manually. You will then want to use a build tool to manage compiling all those source files.\nsbt is such a tool. There are other tools too, some other well-known build tools that come from the Java world are Ant and Maven.\nHow it works is that you create a project file that describes what your project looks like; when you use sbt, this file will be called build.sbt. That file lists all the source files your project consists of, along with other information about your project. Sbt will read the file and then it knows what to do to compile the complete project.\nBesides managing your project, some build tools, including sbt, can automatically manage dependencies for you. This means that if you need to use some libraries written by others, sbt can automatically download the right versions of those libraries and include them in your project for you.\nLet’s look at how to use published libraries to add extra functionality to our apps.\nOpen up build.sbt and add the following line:\nlibraryDependencies += \u0026quot;org.scala-lang.modules\u0026quot; %% \u0026quot;scala-parser-combinators\u0026quot; % \u0026quot;1.1.0\u0026quot; Here, libraryDependencies is a set of dependencies, and by using +=, we’re adding the scala-parser-combinators dependency to the set of dependencies that sbt will go and fetch when it starts up. Now, in any Scala file, you can import classes, objects, etc, from scala-parser-combinators with a regular import.\n","date":1560882600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560882600,"objectID":"e7e86d51e30ae2a0d7e5450a978df169","permalink":"http://sheenamnarula1993.github.io/post/scala/","publishdate":"2019-06-19T00:00:00+05:30","relpermalink":"/post/scala/","section":"post","summary":"SBT is a popular tool for compiling, running, and testing Scala projects of any size. Using a build tool such as sbt (or Maven/Gradle) becomes essential once you create projects with dependencies or more than one code file.\nWhen you write small programs that consist of only one, or just two or three source files, then it\u0026rsquo;s easy enough to compile those source files by typing scalac MyProgram.scala in the command line.","tags":null,"title":"Scala Part-1","type":"post"},{"authors":null,"categories":null,"content":"The cold war was a state of geopolitical tension after second world war between Eastern Bloc(The Soviet Union and its satellite states) and Western Bloc(the United States and its NATO Alies)\nCold war era - 1947 to 1989/1991. But actually it doesnot have any fix date.\nWhy is it called cold war : No direct combat or fight between USSR + Allies and US + allies but there were proxy wars like Korean War,Vietnam War and Afghanistan war.\nProxy wars are like if there are two super powers A(US) and B(Soviet Union), and war is going on between C and D, then A is supporting C and B is supporting D. In this way, they are showing their power to each other indirectly.\nCharacteristics of Cold war :  Ideological differences(US is Capitalist and Soviet is Communist) Atmosphere of political distrust(Each power was thinking that other power wanted to invade in its region) Brinkmanship(means taking world to the edge of war and then take a step back) Psychological warfare(both powers thought that the other power was evil) Arms Race(High military spending) Espionage(to spy other power\u0026rsquo;s activities)  Nuclear Capability : Both powers could destroy earth many times over because of their nuclear capabilities.This was the main reason due to which cold war never turned out to be a hot war.\nThe Beginnings of Cold war : Many people say that the cold war was started with the Russian Revolution in which there was a civil war between Reds(Communists) and Whites.Reds won the war and USSR was established in 1922. USA, UK and France supported Whites at that time.So there were obvious frictions between communists and these nations. US didn\u0026rsquo;t give the recognition to USSR initially but in 1933 Diplomatic Relations between USSR and US got established.\nReason behind Non-Recoginition : On December 6, 1917, the U.S. Government broke off diplomatic relations with Russia, shortly after the Bolshevik Party seized power from the Tsarist regime after the “October Revolution.” President Woodrow Wilson decided to withhold recognition at that time because the new Bolshevik government had refused to honor prior debts to the United States incurred by the Tsarist government, ignored pre-existing treaty agreements with other nations, and seized American property in Russia following the October Revolution. The Bolsheviks had also concluded a separate peace with Germany at Brest-Litovsk in March 1918, ending Russian involvement in World War I. Despite extensive commercial links between the United States and the Soviet Union throughout the 1920s, Wilson’s successors upheld his policy of not recognizing the Soviet Union.\nAfter world war II in 1945, USSR tried to influence throughout central and eastern Europe as a buffer against Germany because in WWI and WWII, Germany attacked Russia and USSR totally understood that in future this could happen again. So to avoid this situation, it thought to create buffer which could be done through eastern and central Europe. On the other side, USA was trying to influence France,West Germany,Japan and Korea and tried to set up Capitalism supporting leaders in these countries. So in this type of atmosphere, Wintson Churchill,the Prime Minister of England, gave a statement \u0026ldquo;An Iron Curtain has descended over the continent\u0026rdquo;.The term Iron Curtain refers to the following picture of map :\nThe Iron Curtain : It was the name for the non-physical boundary dividing Europe into two separate areas from the end of World War II in 1945 until the end of the Cold War in 1991. The term symbolizes the efforts by the Soviet Union to block itself and its satellite states from open contact with the West and its allied states. On the east side of the Iron Curtain were the countries that were connected to or influenced by the Soviet Union, while on the west side were the countries that were allied to the United States or nominally neutral. Separate international economic and military alliances were developed on each side of the Iron Curtain. (West Germany was captured by America,France and Britain and East Germany was occupied by Soviet Union)\nAgainst this Iron Curtain, America opted for some policies.\n  Containment Policy: This policy is about stopping further expansion of Communism.\n  Marshall Plan for Resconstruction: America provided economic help to the countries which suffered a lot of destruction in war. In this way, according to marshall plan, it tried to establish capitalism in those countries.\n  Truman Doctrine,1947: According to this policy, US said that it would support every country which was against communism.It would provide economic,political and even military help to the countries which were against communism.That was why South Korea was supported by US in Korean War.\n  US also propped up many communist regimes throughout the world.\n  Incidents of Cold War : Berlin Blockade (1948) : At that time, West Berlin was under USA and East Berlin was under USSR. The USSR wanted to control entire Berlin.As Berlin was surrounded by USSR, so it stopped the supply route to west Berlin.But USA UK air dropped supplies to break the blockade to maintain west Berlin.For around one year, with the help of two lakh planes, they maintained west Berlin by providing food, fuel etc.So at last, USSR dropped the blockade because it was not successful as air dropped supplies were quite helpful in providing stuff to people.\nIn 1949, Germany was officially divided into two parts East Germany(under USSR) and West Germany(under US)\n1949 Events: THE NORTH ATLANTIC TREATY ORGANIZATION (NATO) Berlin blockade happened in 1948 because of which America was quite aware of the fact that it has to do something about USSR. So USA and some other countries formed NATO. A breif description of NATO is here :\nThe Cold War was in full swing, as the Soviet Union was rising to power, capturing satellite countries. Using their strong dynamic forces, the Soviet Union captured surrounding countries first to help protect them from any invasion. This tactic was used to imprison civilians and force them to join the Soviet military. As their armed forces greatly increased in numbers, other countries and nations feared that the Soviet Union would expand their control and take over other countries.\nIn response to this, the North Atlantic Treaty Organization was formed. NATO is a formal alliance between the territories of North American and Europe. From its inception, its main purpose was to defend each other from the possibility of communist Soviet Union taking control of their nation. Many powerful countries joined NATO by the signing of the official document in 1949: Belgium, Great Britain, Italy, Iceland, Luxembourg, United States, Canada, Netherlands, Denmark, Norway and Portugal. In 1950, General Dwight D. Eisenhower was nominated and appointed as the first supreme allied commander. Since Eisenhower was from the United States, this allowed the U.S. to be a strong force in the organization. West Germany, Turkey and Greece joined by 1955.\nToday, NATO is ideally an outstanding way for the twenty-six different countries and nations to come together. As an organization, the leaders meet with one another to make decisions about security issues and defensive issues against allied attacks. Also, NATO has armed forces, made up of civilians of all twenty-six countries. They defend and aid countries in crisis, just like Darfur. The North Atlantic Council is made up on knowledgeable political and military leaders represented by each country. This council comes to a consensus on making important decisions on what political and military tactics to use, for daily activity.\nResult of civil war in CHINA : In China Civil War, Communist won and China fell under Communist power except Taiwan.\nNuclear Weapon by USSR : In 1949, USSR tested its first nuclear weapon. USA was already having nuclear weapon as we all know about Hiroshima and Nagasaki incident.USA always convinced countries to be with it by showing its nuclear power but now USSR was also having nuclear weapons.This was the main reason why this cold war did not turn out to be hot war.\nMany communist govts were established in east and central Europe\nBy 1950, it was quite clear that the two sides were at war without open conflict and basically it was an ideological war between communism and capitalism.\nNuclear Angle : First time in human history,humans were having that power which could destroy human race and earth like 50 times. Initially, nuclear bombs needed to be carried in planes to attack like in case of Hiroshima and Nagasaki, America sent its planes carrying nuclear bombs to attack.But during this cold war,BALLISTIC MISSILES were developed which could carry nuclear bombs over thousands of killometres.ICBM(Inter Continent Ballistic Missiles) assured mutual assured destrucion which is termed as MAD, means if USA attacked USSR, then in return USSR could also attack USA and both had to suffer same losses.\nKorean war(1950-1953) : It was a first proxy war between these two nations.After world war II, North Korea was under USSR and South Korea was under USA.In 1950,there was a war between North Korea and South Korea. USSR and PRC backed North Korea attacked South Korea. USA and allies came to aid south and a stalemate ensued.(stalemate means no party won and border was still the same). After this, a Cease fire agreement was signed between North and South Korea.\nDomino Theory : After Korean war, domino theory came into being. According to this theory, communism is like a domino.If one country fell in a domino, then due to its effect other countries would also fall.Like in 1950, communism was established in China,north Vietnam,Korea,Laos and USA was afraid of that communism could also be established in Thailand, Malaysia, India, Burma etc.So USA thought to stop the domino effect, they needed to start a proxy war and Vietnam war was the result of this thought.\n1950s Escalation : In decade of 1950, cold war was at its height.\n1953 : Stalin died and a new leader for USSR was Nikita Khrushchev and in USA elections, Eisenhower was chosen by public as a new President.\n1955 : USSR signed a WARSAW pact with communist countries agains NATO.\nAt this time, along with arms race, Space race also began.First satellite and first person to space was sent by Soviet Union but in moon race, America won.\nIn 1950s, USSR was supporting communist rebels in Latin America,Asia and Africa.\nUSA showed a different face to world in 1950s. Just to stop communism, it also supported dictatorship in many countries like Iran,Chile,Brazil,Dominica etc.For this support, USA\u0026rsquo;s reputation got affected because USA was a champion of democracy but it supported dictatorship to stop communism.\n1956 Hungarian Revolution : Hungarian Revolution crushed by soviets by sending military.It was the first time, USSR sent its army in other country to supress revolution.\n1956 Suez crisis : There was a Suez canal which existed in Egypt but was under England because it built this canal. Egypt govt. wanted to nationalize it. So at that time, England, France and Isryl attacked on Egypt but at the mean time USSR participated and backed Egypt.USSR threatened that it could be a nuclear war.In response to this threat, USA pressurized England, France and Isryl to take back their armies from Egypt.\n1950s War in Indochina raged on : War was going on in France colonies and Communist governments supported the local people there against the France government.South Vietnam was supported by America.\n1960s Three World Order : When there were two groups in the world, a third group also came into being which was of Non-Aligned countries whose leaders were India, Egypt, Indonesia and Yugoslavia.Non-aligned movement came into being in 1961.According to this, the countries being a part of this movement were neither supporting US nor USSR.\nWith this a terminology,THREE WORLD ORDER came into being. According to this First World was Capitalist Countries(Western Bloc),Second World was Communism(Eastern Bloc) and Third World included Non-aligned countries.\n1960 U2 plane incident : USA sent a plane over USSR to spy.That plane got crashed and pilot was arrested by USSR.There was a summit going to be held in Paris to find a solution of cold war.Due to this incident, USSR was annoyed and that summit got failed.\n1961 : Berlin wall was constructed by East Germany to stop people who were moving secretly to West Germany because they did not want to live under communist government.\n1962 Cuban crisis : Cuban missile crisis, (October 1962), major confrontation that brought the United States and the Soviet Union close to war over the presence of Soviet nuclear-armed missiles in Cuba.\nHaving promised in May 1960 to defend Cuba with Soviet arms, the Soviet premier Nikita Khrushchev assumed that the United States would take no steps to prevent the installation of Soviet medium- and intermediate-range ballistic missiles in Cuba. Such missiles could hit much of the eastern United States within a few minutes if launched from Cuba. The United States learned in July 1962 that the Soviet Union had begun missile shipments to Cuba. By August 29 new military construction and the presence of Soviet technicians had been reported by U.S. U-2 spy planes flying over the island, and on October 14 the presence of a ballistic missile on a launching site was reported.\nAfter carefully considering the alternatives of an immediate U.S. invasion of Cuba (or air strikes of the missile sites), a blockade of the island, or further diplomatic maneuvers, Pres. John F. Kennedy decided to place a naval “quarantine,” or blockade, on Cuba to prevent further Soviet shipments of missiles. Kennedy announced the quarantine on October 22 and warned that U.S. forces would seize “offensive weapons and associated material” that Soviet vessels might attempt to deliver to Cuba. During the following days, Soviet ships bound for Cuba altered course away from the quarantined zone. As the two superpowers hovered close to the brink of nuclear war, messages were exchanged between Kennedy and Khrushchev amidst extreme tension on both sides. On October 28 Khrushchev capitulated, informing Kennedy that work on the missile sites would be halted and that the missiles already in Cuba would be returned to the Soviet Union. In return, Kennedy committed the United States never to invade Cuba. Kennedy also secretly promised to withdraw the nuclear-armed missiles that the United States had stationed in Turkey in previous years. In the following weeks both superpowers began fulfilling their promises, and the crisis was over by late November. Cuba’s communist leader, Fidel Castro, was infuriated by the Soviets’ retreat in the face of the U.S. ultimatum but was powerless to act.\n1962 Sano Soviet separation : China got separated from USSR saying that ideology behind the communist government of USSR is different from the ideology it is following being a communist country. But both continued to support gorillas in Vietnam war.\n1968- Praque spring reforms crushed by USSR. Vietnam war carried on at a brutal scale.Finally,North Vietnam won backed by USSR won in 1975.\n1970s De-Escalation 1972 : US and China established diplomatic relations, trade relations and America gave recognition to China\n1973- Detente(Cooperation) between USA and USSR : Under this, SALT(Strategic Arms Limitation Talks) was successful in which two topics were discussed : -Reducing nuclear missiles -Reducing anti ballistic missiles\nAfter SALT, tension between two nations was significanly reduced. But in 1979, USSR send troops to Afghanistan to help newly formed Communist govt. which annoyed USA as USA thought that USSR was following its old policy of expansion. So in return, USA supported Mujaheedin(rebels in Afghanistan) against USSR.So this again ended cooperation between USSR and USA.\n1980s 1979 : In 1979, due to end in cooperation between USSR and USA, USA boycotted olympics held in Moscow in 1980 and USSR boycotted olympics held in Los Angeles in 1984.\n1981 : In 1981,Ronald Regan became the President of USA and he spent on military immenselt which could not be competed by USSR due to it failing economy.\nUSSR was shocked about USA\u0026rsquo;s Strategic Defensive Initiative plan under which satellites were supposed to be equipped with nuclear missiles.This was named as STAR WARS by people.But later this plan was not finalized by USA.\n1983 : Large NATO military exercise made USSR to think about USA was gonna attack USSR and brought the world on brink if nuclear annihilation.\nSouth Korean civilian plane shot down by USSR giving excuse that this plane was sent to spy in USSR.\n1985- Mikhail Gorbachev becomes Gen. Sec.of Communist party in USSR. After his arriavl, cold war started to end.\nThe end of Cold War Main reason- reforms brought by Gorbachev and economy of USSR.\nThe communist economy simply could not keep up with high growth rates of capitalist ones because of non-existence of privatization of industries.To increase the economic growth, Gorbachev brougt reforms which were :\nGlasnost : It included:\n Openness Freedom of speech Easing of media censorship(now news from America and other parts of the world could be seen by people) Earlier records declassified by RTI  Perestroika Restructuring/Reformation was done under which Soviet Political and Economic structure was improved.\nThe most important part of Political reforms was Elections.Elections was not mean that there were different parties but it was selection of communist party members by people, initially which was done by higher authorities of communist party.\nForeign Business and privately owned business were allowed under economic reforms.\nDemand of Independence After these reforms,all over Europe(the satellite states or warsaw states), started to demand independence and change in communist government.\nIn 1989, Peaceful revolutions were made to get rid of communism in most countries except Romania where the leader,Nicolae Ceaușescu was shot dead by people.\nUnlike in Hungary and Prague, this time there was no military suppression by Soviet Union to crush these revolutions.\nBerlin wall fell and East and West Germany got united.\nIn response to Gorbachev\u0026rsquo;s reforms, USA started talks for de escalation,limiting nuclear weapoins and removal of army from Afghanistan.\nIn Dec. 1991, USSR is dissolved due to internal revolution and all 15 sub national soviets form their soverign countries.\nAftermath of Cold War  Unipolar world- USA the only Superpower Russian Economy suffered badly Conflicts in Balkan states - Yugoslavia(1992 - 97(approx.)) The threat of nuclear war is still there but very much less than during the span of cold war.  ","date":1559586600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559586600,"objectID":"a7193a1c548a7d8780adc9cae68f66e4","permalink":"http://sheenamnarula1993.github.io/post/cold-war/","publishdate":"2019-06-04T00:00:00+05:30","relpermalink":"/post/cold-war/","section":"post","summary":"The cold war was a state of geopolitical tension after second world war between Eastern Bloc(The Soviet Union and its satellite states) and Western Bloc(the United States and its NATO Alies)\nCold war era - 1947 to 1989/1991. But actually it doesnot have any fix date.\nWhy is it called cold war : No direct combat or fight between USSR + Allies and US + allies but there were proxy wars like Korean War,Vietnam War and Afghanistan war.","tags":null,"title":"Cold War","type":"post"},{"authors":null,"categories":null,"content":"(NO DEFINITIONS ,ONLY UNDERSTANDING) We all have heard the term, Electricity.\nBut when we need to explain this term, we can see the working fan, glowing tube and yes, after plugging in phone\u0026rsquo;s charger, simply we can see the increasing percentage sign. Have you ever tried to explore, what is this Electricity ? What does this contain due to which phone\u0026rsquo;s battery is charging and i am able to use it in daily life and yes, what is this term charging means ?\nSo for this, we need to know about the term \u0026ldquo;Charge\u0026rdquo;.\nCharge : We all at one point must have observered a very common phenomena of rubbing a silk cloth with glass rod or when you comb your hair or take off your hat on a cold, dry day, your hair stand on the end.\nSo what is the reason behind this ?\nLet\u0026rsquo;s take an example of silk cloth and glass rod. Take two pair of glass and silk cloth. When you rub each glass rod with its respective cloth, you will observe that after rubbing, one glass rod will repel another glass rod and same is the case with silk cloth i.e. one silk cloth will repel another silk cloth.\nBut glass rods will start attracting their respective silk clothes.\nWhen this phenomena was observed, in old times, it was said that there were inivisible \u0026ldquo;fluids\u0026rdquo; which were flowing from one object to another and were able to effect a physical force over a distance.\nLater, Charles Dufay was one of the early experimenters who demonstrated that there were definitely two different types of changes wrought by rubbing certain pairs of objects together. The fact that there was more than one type of change manifested in these materials was evident by the fact that there were two types of forces produced: attraction and repulsion. The hypothetical fluid transfer became known as a \u0026ldquo;Charge\u0026rdquo;.\nLater,one researcher, Benjamin Franklin,came to the conclusion that the two types of invisible fluids were not actually two, it was only one type of fluid that is tranferred when two materials are rubbed together, and that the two different “charges” were nothing more than either an excess or a deficiency of that one fluid.After experimenting with silk cloth and glass rod, he observed that glass rod removed some of invisible fluid from silk cloth, which created deficiency of fluid in silk cloth and attraction force is because silk cloth wanted to regain its fluid and trying to achieve its neutral state.\nSo following Franklin’s speculation of the silk cloth rubbing something off of the glass rod, the type of charge that was associated with rubbed silk cloth became known as “negative” (because it was supposed to have a deficiency of fluid) while the type of charge associated with the rubbing glass rod became known as “positive” (because it was supposed to have an excess of fluid).\nAs further experiments proceeded and the concept of atom came into light, it was discovered much later that this “fluid” was actually composed of extremely small bits of matter called \u0026ldquo;Electrons\u0026rdquo;.\nYou must be wondering about the term \u0026ldquo;Electrons\u0026rdquo;.Let\u0026rsquo;s have a very small overview of that as well.\nTill now,that all objects are composed of extremely small “building-blocks” known as atoms and that these atoms are in turn composed of smaller components known as particles. The three fundamental particles comprising most atoms are called protons, neutrons and electrons. Atoms are far too small to be seen, but if we could look at one, it might appear something like this:\nIn this image, the structure of atom can be observed. Here protons and neutrons are tightly coupled with each other forming nucleus, but electrons are loosely coupled due to a lot of empty space between nucleus and electrons.\nNote: protons give the unique identity to atom.If we can remove proton from atom, we can change one element to another.\nAs electrons have significantly more freedom to move around in an atom than either protons or neutrons. In fact, they can be knocked out of their respective positions (even leaving the atom entirely!) by far less energy than what it takes to dislodge particles in the nucleus. (Like in case of silk and glass rod, electrons from glass rod got knocked out by silk cloth) If this happens, the atom still retains its chemical identity, but an important imbalance occurs. Electrons and protons are unique in the fact that they are attracted to one another over a distance. It is this attraction over distance which causes the attraction between rubbed objects, where electrons are moved away from their original atoms to reside around atoms of another object.\nElectrons tend to repel other electrons over a distance, as do protons with other protons.\nNow due to lack of electrons,effective force beacuse of protons is more and force of attraction comes into play as a result of trying to gain the former balance.On other side,the materials which already have electrons in excess amount repel each other.\nThe only reason protons bind together in the nucleus of an atom is because of a much stronger force called the strong nuclear force which has effect only under very short distances.\nBecause of this attraction/repulsion behavior between individual particles, electrons and protons are said to have opposite electric charges. That is, each electron has a negative charge, and each proton a positive charge. In equal numbers within an atom, they counteract each other’s presence so that the net charge within the atom is zero. This is why the picture of a carbon atom has six electrons: to balance out the electric charge of the six protons in the nucleus. If electrons leave or extra electrons arrive, the atom’s net electric charge will be imbalanced, leaving the atom “charged” as a whole, causing it to interact with charged particles and other charged atoms nearby.\nStatic Electricity The result of an imbalance of this “fluid” (electrons) between objects is called static electricity. It is called “static” because the displaced electrons tend to remain stationary after being moved from one insulating material to another.\nAn object whose atoms have received a surplus of electrons is said to be negatively charged, while an object whose atoms are lacking electrons is said to be positively charged.\nMichael Faraday proved (1832) that static electricity was the same as that produced by a battery or a generator.\nFlowing electrons actually called electricity but how these electrons flow and what makes these to flow, is covered in second part of basic electronics.\nSummary:  Electricity comprises of electrons which flow from one object to another when two objects are rubbed together. Object having surplus of electrons is negatively charged and object which is lacking electrons is positively charged.(This charge convention is quite opposite to its explanation, but by the time truth got revealed, Franklin\u0026rsquo;s nomenclature was well established and it was not changed). After moving from one object to another, electrons are stationary at those places and gives us static electricity like a battery(cell).  ","date":1559327400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559327400,"objectID":"3feb18b7ee1373a3e984db4e060c8ffa","permalink":"http://sheenamnarula1993.github.io/post/basic-electronics/","publishdate":"2019-06-01T00:00:00+05:30","relpermalink":"/post/basic-electronics/","section":"post","summary":"(NO DEFINITIONS ,ONLY UNDERSTANDING) We all have heard the term, Electricity.\nBut when we need to explain this term, we can see the working fan, glowing tube and yes, after plugging in phone\u0026rsquo;s charger, simply we can see the increasing percentage sign. Have you ever tried to explore, what is this Electricity ? What does this contain due to which phone\u0026rsquo;s battery is charging and i am able to use it in daily life and yes, what is this term charging means ?","tags":null,"title":"Basic Electronics Part-1","type":"post"}]