[{"authors":["admin"],"categories":null,"content":"This is my website that is a bit copy of me. I am a Software Engineer possessing an experience of 4.5 years as a backend developer(Node js).\nBeing from Electronics backround, I do have a decent knowledge of Electronics fundamentals and would love to go for IOT projects.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"http://sheenamnarula1993.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"This is my website that is a bit copy of me. I am a Software Engineer possessing an experience of 4.5 years as a backend developer(Node js).\nBeing from Electronics backround, I do have a decent knowledge of Electronics fundamentals and would love to go for IOT projects.","tags":null,"title":"Sheenam Narula","type":"author"},{"authors":null,"categories":null,"content":"In this post, we will understand the different parts of configuration file of kubernetes by deploying a node application. For this post, you can find github code at :\nhttps://github.com/sheenamnarula/Node-mongo-kubernetes Refer this repository that contains dockerised node application code and configuration files for kubernetes deployment for better understanding.\nSecond Option : Create a node app and dockerise it. For dockerisation of node app, you can follow below post :\n http://sheenamnarula93.com/post/express-docker-aws-1/ Kubernetes Configuration File : Folder Location for files in cloned repository : k8s\n Node app file : k8s/node-app.yaml Mongo file : k8s/mongodb.yaml Explanation of files : apiVersion : apps/v1  This line describes the Kubernetes API versions to be used to create the resource.  kind : Deployment  This line describes the type of resource to be created.  metadata : labels : app : node-app name : node-app Metadata :  attaches meta-information regarding the resource such as its name or labels.  spec: replicas : 1 selector : matchLabels : app : node-app-pod template : metadata : labels : app : node-app-pod spec : containers : - name: node-application env : - name : DB_URI value : mongodb://mongodb-service:27017/node-application image : sheenam1993/node-application ports : - containerPort: 3007 restartPolicy: Always Spec  describes the specifications of resources  The deployment resources are used to manage and control the life cycle of applications Pods. As a result,the spec section for the deployment resource is containing information about the applications Pod and how to control them, below is a brief description of the basic sections that are required for Deployment Spec:\n  Replicas:   an integer that specifies how many pods the deployment should create.\n  Selectors:   this configuration section specifies the conditions used to select the Pods managed by Deployment. For instance, in the example that we are going to use, Pods that have the label app: mongodb-pod will be managed by the Deployment resource.\nTemplate : this section contains the configurations of the Pods.\nThe Template section contains the following subsections:\n  Metadata   The information that will be attached to every Pod created by our Deployment. The labels defined in this section must match the ones used in the selector sections so that the deployment can manage the Pods after their creation.\n  Spec   The specifications of the Pod containers (notice that containers sections is a list and can include more than one container definition). Here we can define specific container configurations such as the image name, the attached volumes, restart policy and used ports.\nService in Kubernetes apiVersion: v1 kind: Service metadata: labels: app: node-app-pod name: node-app-service spec: ports: - port: 8080 targetPort: 3007 nodePort: 30007 selector: app: node-app-pod type: NodePort A service makes pods available for interaction to other application pods in the cluster.\nExplanation of Service Code :\nkind: Service Kind describes the type of resource to be created. So while creating service, its value is \u0026ldquo;Service\u0026rdquo;\nmetadata: labels: app: node-app-pod name: node-app-service Metadata describes the information about the service. Here we have given two thing under meta data : labels and name.\nA service can be called by another service in cluster by name.\nspec: ports: - port: 8080 targetPort: 3007 nodePort: 30007 selector: app: node-app-pod type: NodePort Spec describes specifications of servoice. Here is description of spec code :\n  ports   ports contain 3 parts further : port : exposed port of service (for other services)\n  targetPort   exposed port of a pod (means on which our app is running)\n  nodePort   exposed port to access service from outside the cluster (means if we have to expose the service to end user to run, we have to use nodePort)\n  selector   the conditions used to select Pods managed by the service resource.\n  type :   Kubernetes supports a couple of service types for managing internal and external traffic. Since MongoDB should not be accessed by any external entity, we will create a ClusterIP service that will expose the service on a cluster-internal IP (only accessible by Pods in the same cluster).But for Node app we have created NodePort service type which can be accessed from outside the cluster as well.\n","date":1602786600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602786600,"objectID":"c2cea20281eedda7d274dd58974a0eb1","permalink":"http://sheenamnarula1993.github.io/post/kubernetes-part-3/","publishdate":"2020-10-16T00:00:00+05:30","relpermalink":"/post/kubernetes-part-3/","section":"post","summary":"In this post, we will understand the different parts of configuration file of kubernetes by deploying a node application. For this post, you can find github code at :\nhttps://github.com/sheenamnarula/Node-mongo-kubernetes Refer this repository that contains dockerised node application code and configuration files for kubernetes deployment for better understanding.\nSecond Option : Create a node app and dockerise it. For dockerisation of node app, you can follow below post :\n http://sheenamnarula93.","tags":null,"title":"Kubernetes Configuration file","type":"post"},{"authors":null,"categories":null,"content":"Get started with Kubernetes on local system To get started with kubernetes on your local system, you can use minikube. Minikube is basically local kubernets which helps to learn and develop kubernetes locally.All you need is Docker (or similarly compatible) container or a Virtual Machine environment\nInstallation process and important commands of minikube is given as below :\nInstallation of minikube on Linux curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux- sudo install minikube-linux-amd64 /usr/local/bin/minikube Command to Start cluster minikube start Command to Stop cluster minikube stop Command to Delete cluster minikube delete --all Minikube Dashboard There are commands to manage and access cluster but for ease, we will use dashboard provided by minikube where each and every detail of cluster can be seen and managed. To launch minikube dashboard, run the following command in terminal :\nminikube dashboard Install kubectl (Command line interface) The Kubernetes command-line tool, kubectl, allows you to run commands against Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs.\nsudo snap install kubectl --classic Commands to create kubernetes objects(after describing yaml files): kubectl apply -f ${fileName}.yaml kubectl create -f ${fileName}.yaml This was very basic understanding of minikube i.e local kubernetes. In the next part, we will look into the kubernetes configuration file. For this, we will deploy a Node application for better understanding of concepts. ","date":1602786600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602786600,"objectID":"dd9cf7b3b5cd646e2b11cb8a67e0c554","permalink":"http://sheenamnarula1993.github.io/post/kubernetes-part-2/","publishdate":"2020-10-16T00:00:00+05:30","relpermalink":"/post/kubernetes-part-2/","section":"post","summary":"Get started with Kubernetes on local system To get started with kubernetes on your local system, you can use minikube. Minikube is basically local kubernets which helps to learn and develop kubernetes locally.All you need is Docker (or similarly compatible) container or a Virtual Machine environment\nInstallation process and important commands of minikube is given as below :\nInstallation of minikube on Linux curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux- sudo install minikube-linux-amd64 /usr/local/bin/minikube Command to Start cluster minikube start Command to Stop cluster minikube stop Command to Delete cluster minikube delete --all Minikube Dashboard There are commands to manage and access cluster but for ease, we will use dashboard provided by minikube where each and every detail of cluster can be seen and managed.","tags":null,"title":"Kubernetes Local System Set up","type":"post"},{"authors":null,"categories":null,"content":"There are times when we want to mount a local file or directory(i.e. a file or directory that exists on host machine / local machine) to pod. To achieve this we use \u0026ldquo;hostpath\u0026rdquo; volume mount technique in kubernetes.\nSteps to mount file using hostpath :\n Mount file/directory from host to pod. Mount file/directory from pod to container(where we actual wanna use this.)  Let us take an example, where we want to mount a file named credentails.json to container at path \u0026ldquo;usr/share/myApp\u0026rdquo;.\nStep 1 : volumes: - name: myfile hostPath: path: home/sheenam/credentials.json type: FileOrCreate Above code refers to the step where we have to mount file to pod. So in pod spec, in volumes section, we are assigning a name which can be further used in container spec while mounting. In hostPath, we are giving the path where credentials.json exists on local machine and type as file as we are mounting file.\nStep 2 : volumeMounts: - mountPath: usr/share/kibana/gcp-key.json name: myfile Above code refers to the step where we have to mount file from pod to container. So in container spec, in volumeMounts section, we are giving mountPath(path in container where we want to keep that file) and name refers to the name mentioned in pod spec (to point to the mounted file in pod from host in step 1).\n","date":1602786600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602786600,"objectID":"5120d68c0031b6cb3c1c632e66a2dede","permalink":"http://sheenamnarula1993.github.io/post/mount-vol-hostpath/","publishdate":"2020-10-16T00:00:00+05:30","relpermalink":"/post/mount-vol-hostpath/","section":"post","summary":"There are times when we want to mount a local file or directory(i.e. a file or directory that exists on host machine / local machine) to pod. To achieve this we use \u0026ldquo;hostpath\u0026rdquo; volume mount technique in kubernetes.\nSteps to mount file using hostpath :\n Mount file/directory from host to pod. Mount file/directory from pod to container(where we actual wanna use this.)  Let us take an example, where we want to mount a file named credentails.","tags":null,"title":"Kubernetes Mounting file or directory","type":"post"},{"authors":null,"categories":null,"content":"Kubernetes Vocab : Node : It is an instance of computer which is also known as worker nodes.\nPods : It is a smallest unit of kubernetes that we can deploy on kubernets cluster.It contain container\u0026rsquo;s image. Bascially a pod is a collection of more than one conatiners which are supposed run on one host logically.\nWhen a pod is created, an ip address is assigned to it.When a worker node dies, replica of that pod is created but with different ip address and different volumes. That means everything related to pod lifecycle dies with that pod.(stable ip address concept)\nServices : Services are the end points that export ports to outside world and map the ports to pods.\nDeployment : It represents a set of multiple identical pods.Deployments run multiple instances or replicas of your application.It ensures the availability of one or more instances of app to serve user requests.Deployments are managed by kubernetes deployment controller. Deployments use a Pod template, which contains a specification for its Pods. The Pod specification determines how each Pod should look like: what applications should run inside its containers, which volumes the Pods should mount, its labels, and more.\nWhen a Deployment\u0026rsquo;s Pod template is changed, new Pods are automatically created one at a time.\nKUBERNETES ARCHITECTURE : Kubernetes Master Components: Etcd, API Server, Controller Manager, and Scheduler : Kubernetes master is considered as the brain of kubernetes cluster. It run Etcd, Scheduler, Controller Manager and Api Server inside it.Lets discuss there components briefly so that working on kubernetes should not appear to be strange and knowing components of kubernetes master will help us in a significant way in creating a working image of kubernetes master.\nEtcd : Etcd is a very important part of kubernetes master node. It saves the configuration data of kubernetes cluster like which nodes are there,how many pods are there and which pod is running on which node and a lot more information regarding cluster state. Kubernetes master reads and write data to etcd to get the current state and decides the required logical operations to be performed to achieve the desired state. Etcdctl is a command line interface to manage Etcd through which many operations for etcd can be performed like adding and removing key-value pairs(etcd data storage format is key value pair), verify cluster health, generating database snapshots(As Kubernetes master is highly dependent on etcd for data, so we should have a backup plan for etcd. We can save a snapshot of etcd by using etcdctl save snapshot command).\nEtcd also impelements a \u0026ldquo;watcher\u0026rdquo; feature in which for every key there is a watcher. Whenever key changes, watcher is notified and in return an event is triggered for Api server. After listening to event, api server takes important decisions of operations to be performed to move current state towards desired state.\nApi Server : (The REST API is the fundamental fabric of Kubernetes. All operations and communications between components, and external user commands are REST API calls that the API Server handles. Consequently, everything in the Kubernetes platform is treated as an API object and has a corresponding entry in the API.) Kubernetes api server is a main management point of kubernetes cluster.Basically when we interact with kubernetes using kubectl command line interface, we are actually communicating with Api server component. Only Api server component directly communicates with Etcd to read, write and update objects after validating requests. Rest all components has to go through Api server to work with cluster state. The API Server also implements a watch mechanism (similar to etcd) for clients to watch for changes. This allows components such as the Scheduler and Controller Manager to interact with the API Server in a loosely coupled manner. For example :\n kubectl writes to api server Api server writes to etcd after validating request. etcd notifies back to api server. Api server invokes scheduler. Scheduler decides where to run the pod on and return back to Api server. Api persists data in etcd. Etcd writes back to api server. Api server invokes the Kubelet in corresponding node. Kubelet talks to docker daemon using the api over docker socket to create continer. Kubelet updates the status to Api server. Api server interacts with etcd to persist new state.  Controller Manager : Controller manager watches the cluster state through Api server and when it gets notified, it makes necessary changes to move current state of cluster towards desired state.For e.g. Replication controller always maintains the required number of replicas of pods.\nScheduler : Scheduler watches for unscheduled pods and decides as which pod has to run on which node. Once a pod\u0026rsquo;s binding to node is done, regular task of Kubelet is invoked which in turn calls container engine like docker to create and start container.\nKubernetes Node Components : Basically, Kubernetes node runs Kubelet and Service Proxy components. It also runs container engine like Docker which in turn runs containerized applications.\nKubelet : It\u0026rsquo;s an agent that runs on each node and being a watcher it reports activities of pods to api server.It makes sure that pods attached to its node are running. It talks to docker daemon through docker socket api and which in turn is responsible for running containers.\nJobs of kubelet :\n Run the pods. Report the status of node and pods to api server. Run container probes.  cAdvisor The Kubelet ships with built-in support for cAdvisor, which collects, aggregates, processes and exports metrics (such as CPU, memory, file and network usage) about running containers on a given node. cAdvisor includes a built-in web interface available on port 4194 (just open your browser and navigate to http://:4194/).\nYou can view the cAdvisor /metrics endpoint by issuing a GET request to http://:4194/metrics.\nService Proxy : Service Proxy runs on each node and is responsible for watching the api server for pods and services definitions' changes to keep the entire network\u0026rsquo;s configuration up to date,ensuring that one pod can talk to another pod,one node can talk to another node, one container can talk to another container.In addition to this, it maintains all ip route tables so that it can redirect user request to a correct backend resource. That is why, when we hit the Node port, even if app is not available on that node but it redirects the request to the correct backend resource for execution.\n","date":1602700200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602700200,"objectID":"65197beba3fe29b1ae29ee33b3571e2e","permalink":"http://sheenamnarula1993.github.io/post/kubernetes-part-1/","publishdate":"2020-10-15T00:00:00+05:30","relpermalink":"/post/kubernetes-part-1/","section":"post","summary":"Kubernetes Vocab : Node : It is an instance of computer which is also known as worker nodes.\nPods : It is a smallest unit of kubernetes that we can deploy on kubernets cluster.It contain container\u0026rsquo;s image. Bascially a pod is a collection of more than one conatiners which are supposed run on one host logically.\nWhen a pod is created, an ip address is assigned to it.When a worker node dies, replica of that pod is created but with different ip address and different volumes.","tags":null,"title":"Kubernetes Architecture","type":"post"},{"authors":null,"categories":null,"content":"Syntax of import/export : file : exportVar.js\nexport const m = 1 ; file : importVar.js\nimport {m} from exportVar.js Syntax of require : file : exportVar.js\nconst m = 1; module.exports = m ; file: importVar.js\nconst m = require('./exportVar.js') Loading Strategy : Mainly import/export i.e. ES6 structure, involves asynchronous loading of modules. But in case of require(), synchronous loading of modules takes place. It means, if a file needs to load 5 modules, in case of ES6 structure, second module won\u0026rsquo;t wait for first module to be loaded completely.But in case of require, loading of second module will start after loading of first module is completed.\nThis is the key difference between import/export and require.\nTo get more deep in to this, we need to know how ES6 structure is working and how it is different from Common js.\nThe key difference between the Common JS and ES6 modules is when to know the shape of module.\nIn Common JS, when a module is exported, the steps performed before execution is given in this figure.\nHere the common mistake that most of the developers make in understanding the loading step. In loading step, it is determined that what kind of thing is pointed by absolute path that is obtained on completion of resolution step.If it is Node.js native module, then dynamic linking of module is decided. On the other hand, if it is JSON file, the content is loaded in memory for further use. Here is the trick, in wrapper step, before evaluation, the loaded Javasacript is wrapped in a function. For e.g.\nIn file, example.js\nconst m = 1; module.exports.m = m; code is actually wrapped and passed to evaluation in below function format :\nfunction (exports, require, module, **filename, **dirname) { const m = 1; module.exports.m = m; } Here the exports is a parameter for wrapper function and it is a normal javascript object. So when the wrapper function is evaluated, it returns the exports object which is then used as a result of require(). The main point to note here is that, there is no way to determine as what kind of module is being exported by Common JS until the evaluation of wrapper.\nOn the other hand, if we use import/export i.e. ES6 feature, the equivalent code of above example is :\nIn file, example.js\nexport const m = 1; import {m} from './example.js' In this case, the shape of m is actually determined at the parsing step i.e before evaluation step. Actually, while parsing, and before evaluation, a Module Record is created, where a static listing of modules that have been exported in code are listed and verified. That means, it is verified first whether the exported m exists or not and link between import file and exported module i.e. m is eastablished. Only after creation of this module record, code is actually evaluated. Here the key point is, before even evaluation of code, the shape of module is known or we can say that, import/export is resolved before evaluation of code.\nChallenge for Node.js : Here is the challenge for Node.js comes up when exported module is not an ESM i.e ECMASCRIPT Module but a Common JS module because while importing with import statement or according to ES6 structure, shape of module should be known before evaluation but in case of Common JS module it becomes impossible to determine shape of code before evaluation step due to wrapper function.\nTo resolve this challenge, one proposal came up in which it was proposed that if a module is ESM module, then it will be directly added to list of Module Record but if it is not i.e if it is a Common JS module, then rather than giving error or giving up, the verification step will be left in pending state and a process of evaluation of that module will be executed. After getting confirmation of existence of that module, Module Record will be created.\nBut again for the implementation of this proposal, Node js requires some mechanism to identify whether the module is ESM or Common JS Module. Various approaches have been suggested but Michael Jackson Script is supposed to be accepted.\nMichael Jackson Script : In this a new file extension *.mjs is supposed to be introduced for ESM modules. Right now, Node is distinguishing between the modules like this :\n.node files : native modules\n.js : Common JS modules\n.json : JSON files\nNow *.mjs will specify that the module is ESM and it should be loaded with the right mechanism. The key point here is by knowing the extensions of file, loading mechanism will be selected. For Common JS module, evaluation of module will be done first so that verification of existence can be done.\nE.g. There are two files common.js and esm.mjs\nimport c from './common.js' will be treated as Common js module\nimport e from './esm.mjs' will be treated as ESM module\nWhat to do if i want to use ES6 import/export feature in project? Answer is Babel If we want to use import/export feature of ES6, we need to use babel which transpiles code in Common JS. Here is a point which needs to be understood that Babel only converts the syntax to Common JS syntax so that it can be resolved by Node js, but implementation is according to ESM only.\nExample: If we see named imports like\nimport {a,b} from 'ab'; This syntax will work only if ab is an ESM. But if it is Common JS module, it is not possible to use this syntax as shape of code can\u0026rsquo;t be determined until evaluation is done in this case.\nBut here BABEL does a perfect job for us by converting ES6 syntax to Common JS syntax and we get the required implementation.\nSignificance of using ES6 structure : In case of ES6 structure, the first step of loading the contents of file from disk is quite similar but may happen asynchronously. When the content of files is available, they are parsed. While parsing, the shape of exported module is determined and linked to respective import.That means all exports and imports will know their targets before evaluation itself i.e. they are resolved before execution step and that too happen asynchronously. In node.js terms, we can say that loading scripts, resolution of import and export and evaluation of module code occur over multiple turns of the event loop.\nPractical Example : module a and b, both does not exist. b is imported using require and a is imported using import. Now due to use of require, existence of b is not verified and code does not throw error for non-existence of b. But in case of module a, existence of a is verified and code throws error for a at very early stage i.e at parsing step only.\n","date":1564511400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564511400,"objectID":"8a9034665e184fa7b13bc43449ada527","permalink":"http://sheenamnarula1993.github.io/post/import-require/","publishdate":"2019-07-31T00:00:00+05:30","relpermalink":"/post/import-require/","section":"post","summary":"Syntax of import/export : file : exportVar.js\nexport const m = 1 ; file : importVar.js\nimport {m} from exportVar.js Syntax of require : file : exportVar.js\nconst m = 1; module.exports = m ; file: importVar.js\nconst m = require('./exportVar.js') Loading Strategy : Mainly import/export i.e. ES6 structure, involves asynchronous loading of modules. But in case of require(), synchronous loading of modules takes place. It means, if a file needs to load 5 modules, in case of ES6 structure, second module won\u0026rsquo;t wait for first module to be loaded completely.","tags":null,"title":"Import Vs Require","type":"post"},{"authors":null,"categories":null,"content":"Containerization : Containerization is a lightweight alternative to full machine virtualization that involves encapsulating an application in a container with its own operating environment. Containers use the same host operating system (OS) repeatedly, instead of installing an OS for each guest VM.\nVirtualization : A virtual machine is a copy of a complete server basically it has its own OS which is treated as a guest OS on host OS.\nUse case of Containers : Suppose we have two applications written on different versions of same language.It is possible that if we deploy both on same VM, one of those may break due to unsupported version of language.But if we deploy them in Containers,every container will have the required version and both apps will run in isolated environments.\nSecurity Level : Virtual Machines are fully isolated and hence more secure. But in case of containers, process-level isolation and hence less secure.\n","date":1563561000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563561000,"objectID":"bd227681f4b1cb3f0b932d1d56759e4a","permalink":"http://sheenamnarula1993.github.io/post/containerization-vs-vm/","publishdate":"2019-07-20T00:00:00+05:30","relpermalink":"/post/containerization-vs-vm/","section":"post","summary":"Containerization : Containerization is a lightweight alternative to full machine virtualization that involves encapsulating an application in a container with its own operating environment. Containers use the same host operating system (OS) repeatedly, instead of installing an OS for each guest VM.\nVirtualization : A virtual machine is a copy of a complete server basically it has its own OS which is treated as a guest OS on host OS.","tags":null,"title":"Containerization Vs Virtualization","type":"post"},{"authors":null,"categories":null,"content":"I am assuming that an amazon account is already set up to use the Amazon Elastic Container Service(ECS).\nSteps to deploy two containers(app + database) -\n Dockerise app Create Registry(ECR) Upload app image to ECR Create task definition with two containers Create a cluster Create a service and run it.  Practical Explanation of steps : 1. Dockerise app for this you can check this practice project : https://github.com/sheenamnarula/node-graphql-docker-aws\n2. Create Registry(ECR) We need to keep image of app somewhere from where it is pulled while starting container.We can do two things here. Either we can use Docker Hub or we can use AWS Elastic Container Registry(ECR) service. In this tutorial,i am using ECR to keep the created images.\nLog in to web console into your prefered region and choose ECR from services.\nThere create a repository to store image of your app.After creating repository, you will see a button \u0026ldquo;View Push Commands\u0026rdquo; on right side. On clicking that button, you will get some commands that will be used to push image of app to this repository. 3 . Upload image to ECR Run the commands, to push the image to ECR.\nCommand 1.  aws ecr get-login --no-include-email --region ap-south-1  This will help you to login to aws through cli.(Note : Install aws-cli to run these commands.) ( Link to install aws cli : https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html )\nCommand 2.  docker build -t node-express .  This command will help you in building image of your app. Run this command in the folder containing the app. -t is to give the name to your image. You can also customize it according to your preference so that you can find the created image easily. Command 3.  docker tag node-express:latest 597357263415.dkr.ecr.ap-south-1.amazonaws.com/node-express:latest  This command is used to tag your created image to the ECR. Command 4 .  docker push 597357263415.dkr.ecr.ap-south-1.amazonaws.com/node-express:latest  This command is used to push the created image to ECR. This command can take few minutes to execute. 3. Create Task definitions As we run docker commands like docker run in docker cli, here task definitions are used for this purpose. Every task consists of - docker containers details, port mappings, network details, environment variables, volumes if any, and the most important thing, image of app. Choose Task Definitions in left navigation pane.\nHere we will define two containers in one task as running app willl contain api container and database container.\nFirst Container : Api Container Copy the url of your image that we pushed in ECR in second step.\nClick on Create New Task Definition and choose EC2 and then click on next. Here you will get a form to define your task. Fill out the required fields and click on Add Containers. In add container form, specify the container name and memory required. Then give the port mappings i.e host port and container port. Give Environment variables using Key-value pair(In this project, we are using mongo db as environment variable) In network links, you can declare the linking of other containers with this container simply by following syntax - othercontainer: alias Here we are running database in second container and we are gonna name it as mongo-container(yet to be defined) We are linking this container to database container. Then click on create and you will get your api container created.\nSecond Container : Database Container For this container, we need a mongo image and this can be directly pulled from Docker Hub. Here is the required url for mongo image : registry.hub.docker.com/library/mongo:latest\nNow we need to be careful while naming second container as we have mentioned in the linking of first container as \u0026ldquo;mongo-container\u0026rdquo;. So for correct linking, we need to keep the name of our db container as \u0026ldquo;mongo-container\u0026rdquo;\nand also mention the hostname as mongo-container.\nNow click on create option and you will get your task definition created with two containers.\n4. Create Cluster : Cluster is where container runs.\nChoose Clusters from left navigation pane and choose create cluster.Creating cluster is like setting up an EC2 instance.After clicking on create cluster, choose EC2+Linux and create cluster with options vpc - create new vpc group,instance type- m4.large and number of required instances. Configuration of cluster : Now you can see the created cluster, on clicking that cluster, you can see tabs for task definition, ec2 instance and services.\n5. Create Service : Service is used to run your defined tasks in cluster. Click on create service and fill the required fields.\nCreating service : Choose the task definition that have been created in step 3 and run the service.\nInitially it will show pending state and soon it will be in running state. When it is in running state, you can go to cluster and then click on ec2 instance. On clicking that, details of that instance will appear. From there copy the public dns. Paste the copied address in browser and you can see your app in running state. ","date":1561746600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561746600,"objectID":"422a0a7a75c8756d4e8ac358bbdc6623","permalink":"http://sheenamnarula1993.github.io/post/express-docker-aws-2/","publishdate":"2019-06-29T00:00:00+05:30","relpermalink":"/post/express-docker-aws-2/","section":"post","summary":"I am assuming that an amazon account is already set up to use the Amazon Elastic Container Service(ECS).\nSteps to deploy two containers(app + database) -\n Dockerise app Create Registry(ECR) Upload app image to ECR Create task definition with two containers Create a cluster Create a service and run it.  Practical Explanation of steps : 1. Dockerise app for this you can check this practice project : https://github.com/sheenamnarula/node-graphql-docker-aws","tags":null,"title":"Node-Docker-Aws Part-2","type":"post"},{"authors":null,"categories":null,"content":"First of all, make a project in node-express-mongo and dockerise it. Here I am giving a github link of a basic node-express app made along with graphql as this has become my favourite language because of its advantage of providing independence to front-end developers.\nGithub link for dockerised app : https://github.com/sheenamnarula/node-graphql-docker-aws Docker File Lets understand first, what is the meaning of lines written in the docker file.\nFrom node:8.9.0 This line specifies the base image.Every docker file starts with a FROM instruction. As we are writing here for node app, so we need to set a node environmnent for the app. Here node:is the base image along with the required version for our app ie. 8.9.0\nWORKDIR /app This line refers to a directory in which we are going to keep our application files.Basically WORKDIR sets a directory where another commands like COPY, RUN, CMD, ADD can be run. In short, it is providing an entry point of app directory.\nCOPY package\\*.json ./ To start a project or run a project, we need to install node modules and as we know in a node app, package.json is a file where dependencies of project is documented. So we are copying package.json in working directory so that further instructions can install node modules using this file. Here package* refers to package.json and package.lock.json files.\nRUN npm install RUN npm install nodemon babel-cli babel-preset-env These both lines contain RUN command. After copying package.json in work directory that is suppose to be entry point of our source files,using RUN command we are installing node modules and required dependencies for our project. Basically RUN command is used for the command that we run in a terminal and then it commits a new image which is further used by next instructions of docker file\nEXPOSE 3007 This line is very important to understand as it defines which port of container is exposed.When we give any port with this command, that means that port of container is exposed.Means if we want to access app, we have to write the hostname simply i.e we need to write port 80 on browser. But if we write publish port in docker run command with expose port that means the app will be accesed through published port. In short, EXPOSE is used to open port for network but publish is used to map the opened port to public accessible port.\nCOPY . /app Here we are copying the all content of app to our container working directory so that app can be run.\nCMD npm run start This line refers to executing the command that is required to start the project.\nDifference between RUN and CMD - Here is an important difference between RUN and CMD :\nRUN - Basically RUN command triggers when we are building docker image. After RUN command, image is committed and next command uses that committed image.\nCMD - Basically CMD command triggers when we launch the created docker image.Dockerfile can have only one CMD command and it can be overriden while starting a container i.e. executing docker run $image $other_command\nThis is all about a docker file. Environment variables can also be included using ENV command. Now this is all about one container which will contain app. But here database required is mongo db and we also need a container for mongo db.\nNOTE : It is possible to run mongo db and app in one container but it will create problem in scaling because if we need to scale app, thereby we will be scaling mongo db as well and moreover, it is difficult to maintain same data availability for all container apps in this case. So we will have one container for mongo db service following microservice architecture i.e. every service is running independently.\nTo manage more than one container we will go for a tool named Compose. Compose is a tool that is used to define and run multi-container docker applications.In this, a YAML file is used to configure multiple containers of application services.Then with a single command, we can create and start all the services using yaml configuration file.\nHere is the code for docker compose file and explanation of each line of code :\nExplanation of Docker Compose file : version: \u0026quot;3\u0026quot; - This line refers to the docker compose file format version.\nservices: This line refers to the start of container configurations.A service definition contains configuration that is applied to each container started for that service.\napp: This line refers to the name of your service.\ncontainer_name: docker-node-mongo By default, a random name is generated. To generate a meaningful name, we use container_name parameter in config.\nrestart: always By default behaviour of container is that it never restarts. By setting restart to always, we are making container to opt restarting behaviour.\nbuild: . This option is used when we have to make an image from a docker file. Relative path is given in this option so that docker can look for Dockerfile according to given path to build image.\\\nports: - \u0026quot;80:3007\u0026quot; This option is used to mention the exposed port of container. HOST:CONTAINER is short syntax to metion ports.\nThe long form syntax allows the configuration of additional fields that can’t be expressed in the short form.\ntarget: the port inside the container published: the publicly exposed port protocol: the port protocol (tcp or udp) mode: host for publishing a host port on each node, or ingress for a swarm mode port to be load balanced.\nports: -target: 80 published: 8080 protocol: tcp mode: host depends_on: - mongo This line express the dependencies of one container to other. To start a container,the containers on which it is dependent, will be started first\nmongo: container_name: mongo image: mongo This will check the mongo image if it is available locally and in case of unavailability, image will be downloaded from docker hub(github for docker images).\nports: - \u0026quot;27017:27017\u0026quot; Here is on more command like depends_on i.e. links. But link is deprecated now. depends_on gives us start order but links just to link one container to another.But now, if containers are placed in same network, they can connect with each other by using their name.\nNow by running a single command, \u0026ldquo;docker-compose up\u0026rdquo;, we can see two containers running one is our app container and other is mongo container.\nNote: In project here is one commented line , mongodb://mongo:27017/expGraphqlDemo. In this mongo is container name and 27017 is container\u0026rsquo;s published port.But for deployment purpose, we are making it configurable and will pass as environment variable while deployment.\n","date":1561660200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561660200,"objectID":"9e0e13f0dc0a854765ba2c500efea3d1","permalink":"http://sheenamnarula1993.github.io/post/express-docker-aws-1/","publishdate":"2019-06-28T00:00:00+05:30","relpermalink":"/post/express-docker-aws-1/","section":"post","summary":"First of all, make a project in node-express-mongo and dockerise it. Here I am giving a github link of a basic node-express app made along with graphql as this has become my favourite language because of its advantage of providing independence to front-end developers.\nGithub link for dockerised app : https://github.com/sheenamnarula/node-graphql-docker-aws Docker File Lets understand first, what is the meaning of lines written in the docker file.\nFrom node:8.9.0 This line specifies the base image.","tags":null,"title":"Node-Docker-Aws Part-1","type":"post"},{"authors":null,"categories":null,"content":"Show commands and Management commands  $ docker  Docker version :  $ docker --version  Show info like number of containers  $ docker info  Commands required for Containers Command to run a container in foreground :  $ docker container run -it -p 80:80 {image name}  Example : docker container run -it -p 80:80 nginx\nCommand to run a container in backround :  $ docker container run -d -p 80:80 {image name}  Short hand command  $ docker container run -d -p 80:80 {image name}  Naming custom names to container  $ docker container run -p 80:80 --name {container name} {image name}  Example : docker container run -d -p 80:80 \u0026ndash;name nginx-server nginx\nPurpose of \u0026ldquo;run\u0026rdquo; in command :  It actually looks for image in local cache. If image is not found, it looks into Docker Hub where default image repo may exist(its like github for docker images). It pulls it down from there and stores into local image cache. Starts in a new container. As we know syntax for port specification is HOST:CONTAINER port. So here we can access on port 80. We can also access it on any port by changing host port. For e.g. , if we want access on port 3000, then we need to write here 3000:80.  Commands for listing containers These two commands gives us running containers.\n $ docker ps $ docker container ls  This command will give us all containers(including those containers as well which are not running).\n $ docker container ls -a  Commands to stop container Stop a specific container  $ docker container stop [ID]  Stop all containers  $ docker stop $(docker ps -aq)  Remove a specific container  $ docker container rm -f [ID]  Remove multiple containers  $ docker container rm [ID] [ID] [ID]  Remove all containers  $ docker rm $(docker ps -aq)  Get logs of container  $ docker container logs [NAME]  List processes running in container  $ docker container top [NAME]  View Container Info  $ docker container inspect [NAME]  Performance stats  $ docker container stats [NAME]  Commands required for Images List images  $ docker images  Build Image  $ docker image build -t [REPONAME] .  Pull Image  $ docker pull [IMAGE]  Remove a specific image  $ docker image rm [IMAGE]  Remove all images  $ docker rmi $(docker images -a -q)  Image tagging and pushing to docker hub Tags are labels that point to an Image\nTagging an Existing Image  $ docker image tag {old name} {new tag name}  Upload to docker hub  $ docker image push {imageTag}  Note : If pushing operation is denied, do login in docker using  $ docker login  Docker Networks Get Port  $ docker container port [NAME]  List Networks  $ docker network ls  Inspect Network  $ docker network inspect [NETWORK_NAME]  Create network $ docker network create [NETWORK_NAME]  Create container on network $ docker container run -d --name [NAME] --network [NETWORK_NAME] [IMAGE_NAME]  Connect existing container to network $ docker network connect [NETWORK_NAME] [CONTAINER_NAME]  Disconnect container from network $ docker network disconnect [NETWORK_NAME] [CONTAINER_NAME]  Detach network from container $ docker network disconnect  ","date":1561314600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561314600,"objectID":"3141febc905c62a73d557fc6f4987809","permalink":"http://sheenamnarula1993.github.io/post/docker-basic-commands/","publishdate":"2019-06-24T00:00:00+05:30","relpermalink":"/post/docker-basic-commands/","section":"post","summary":"Show commands and Management commands  $ docker  Docker version :  $ docker --version  Show info like number of containers  $ docker info  Commands required for Containers Command to run a container in foreground :  $ docker container run -it -p 80:80 {image name}  Example : docker container run -it -p 80:80 nginx\nCommand to run a container in backround :  $ docker container run -d -p 80:80 {image name}  Short hand command  $ docker container run -d -p 80:80 {image name}  Naming custom names to container  $ docker container run -p 80:80 --name {container name} {image name}  Example : docker container run -d -p 80:80 \u0026ndash;name nginx-server nginx","tags":null,"title":"Basic Docker Commands","type":"post"},{"authors":null,"categories":null,"content":"(NO DEFINITIONS ,ONLY UNDERSTANDING) We all have heard the term, Electricity.\nBut when we need to explain this term, we can see the working fan, glowing tube and yes, after plugging in phone\u0026rsquo;s charger, simply we can see the increasing percentage sign. Have you ever tried to explore, what is this Electricity ? What does this contain due to which phone\u0026rsquo;s battery is charging and i am able to use it in daily life and yes, what is this term charging means ?\nSo for this, we need to know about the term \u0026ldquo;Charge\u0026rdquo;.\nCharge : We all at one point must have observered a very common phenomena of rubbing a silk cloth with glass rod or when you comb your hair or take off your hat on a cold, dry day, your hair stand on the end.\nSo what is the reason behind this ?\nLet\u0026rsquo;s take an example of silk cloth and glass rod. Take two pair of glass and silk cloth. When you rub each glass rod with its respective cloth, you will observe that after rubbing, one glass rod will repel another glass rod and same is the case with silk cloth i.e. one silk cloth will repel another silk cloth.\nBut glass rods will start attracting their respective silk clothes.\nWhen this phenomena was observed, in old times, it was said that there were inivisible \u0026ldquo;fluids\u0026rdquo; which were flowing from one object to another and were able to effect a physical force over a distance.\nLater, Charles Dufay was one of the early experimenters who demonstrated that there were definitely two different types of changes wrought by rubbing certain pairs of objects together. The fact that there was more than one type of change manifested in these materials was evident by the fact that there were two types of forces produced: attraction and repulsion. The hypothetical fluid transfer became known as a \u0026ldquo;Charge\u0026rdquo;.\nLater,one researcher, Benjamin Franklin,came to the conclusion that the two types of invisible fluids were not actually two, it was only one type of fluid that is tranferred when two materials are rubbed together, and that the two different “charges” were nothing more than either an excess or a deficiency of that one fluid.After experimenting with silk cloth and glass rod, he observed that glass rod removed some of invisible fluid from silk cloth, which created deficiency of fluid in silk cloth and attraction force is because silk cloth wanted to regain its fluid and trying to achieve its neutral state.\nSo following Franklin’s speculation of the silk cloth rubbing something off of the glass rod, the type of charge that was associated with rubbed silk cloth became known as “negative” (because it was supposed to have a deficiency of fluid) while the type of charge associated with the rubbing glass rod became known as “positive” (because it was supposed to have an excess of fluid).\nAs further experiments proceeded and the concept of atom came into light, it was discovered much later that this “fluid” was actually composed of extremely small bits of matter called \u0026ldquo;Electrons\u0026rdquo;.\nYou must be wondering about the term \u0026ldquo;Electrons\u0026rdquo;.Let\u0026rsquo;s have a very small overview of that as well.\nTill now,that all objects are composed of extremely small “building-blocks” known as atoms and that these atoms are in turn composed of smaller components known as particles. The three fundamental particles comprising most atoms are called protons, neutrons and electrons. Atoms are far too small to be seen, but if we could look at one, it might appear something like this:\nIn this image, the structure of atom can be observed. Here protons and neutrons are tightly coupled with each other forming nucleus, but electrons are loosely coupled due to a lot of empty space between nucleus and electrons.\nNote: protons give the unique identity to atom.If we can remove proton from atom, we can change one element to another.\nAs electrons have significantly more freedom to move around in an atom than either protons or neutrons. In fact, they can be knocked out of their respective positions (even leaving the atom entirely!) by far less energy than what it takes to dislodge particles in the nucleus. (Like in case of silk and glass rod, electrons from glass rod got knocked out by silk cloth) If this happens, the atom still retains its chemical identity, but an important imbalance occurs. Electrons and protons are unique in the fact that they are attracted to one another over a distance. It is this attraction over distance which causes the attraction between rubbed objects, where electrons are moved away from their original atoms to reside around atoms of another object.\nElectrons tend to repel other electrons over a distance, as do protons with other protons.\nNow due to lack of electrons,effective force beacuse of protons is more and force of attraction comes into play as a result of trying to gain the former balance.On other side,the materials which already have electrons in excess amount repel each other.\nThe only reason protons bind together in the nucleus of an atom is because of a much stronger force called the strong nuclear force which has effect only under very short distances.\nBecause of this attraction/repulsion behavior between individual particles, electrons and protons are said to have opposite electric charges. That is, each electron has a negative charge, and each proton a positive charge. In equal numbers within an atom, they counteract each other’s presence so that the net charge within the atom is zero. This is why the picture of a carbon atom has six electrons: to balance out the electric charge of the six protons in the nucleus. If electrons leave or extra electrons arrive, the atom’s net electric charge will be imbalanced, leaving the atom “charged” as a whole, causing it to interact with charged particles and other charged atoms nearby.\nStatic Electricity The result of an imbalance of this “fluid” (electrons) between objects is called static electricity. It is called “static” because the displaced electrons tend to remain stationary after being moved from one insulating material to another.\nAn object whose atoms have received a surplus of electrons is said to be negatively charged, while an object whose atoms are lacking electrons is said to be positively charged.\nMichael Faraday proved (1832) that static electricity was the same as that produced by a battery or a generator.\nFlowing electrons actually called electricity but how these electrons flow and what makes these to flow, is covered in second part of basic electronics.\nSummary:  Electricity comprises of electrons which flow from one object to another when two objects are rubbed together. Object having surplus of electrons is negatively charged and object which is lacking electrons is positively charged.(This charge convention is quite opposite to its explanation, but by the time truth got revealed, Franklin\u0026rsquo;s nomenclature was well established and it was not changed). After moving from one object to another, electrons are stationary at those places and gives us static electricity like a battery(cell).  ","date":1559327400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559327400,"objectID":"3feb18b7ee1373a3e984db4e060c8ffa","permalink":"http://sheenamnarula1993.github.io/post/basic-electronics/","publishdate":"2019-06-01T00:00:00+05:30","relpermalink":"/post/basic-electronics/","section":"post","summary":"(NO DEFINITIONS ,ONLY UNDERSTANDING) We all have heard the term, Electricity.\nBut when we need to explain this term, we can see the working fan, glowing tube and yes, after plugging in phone\u0026rsquo;s charger, simply we can see the increasing percentage sign. Have you ever tried to explore, what is this Electricity ? What does this contain due to which phone\u0026rsquo;s battery is charging and i am able to use it in daily life and yes, what is this term charging means ?","tags":null,"title":"Basic Electronics Part-1","type":"post"}]